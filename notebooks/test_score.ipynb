{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NO_ANS = \"[CLS]\"\n",
    "INVALID_QUESTION = -1\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path) as json_file:\n",
    "        o_file = json_file.read()\n",
    "    return json.loads(o_file)\n",
    "\n",
    "\n",
    "def filter_questions(exp_ans, pred_ans):\n",
    "    \"\"\"\n",
    "    check if the expected answer and the predicted answer are the same.\n",
    "    Args:\n",
    "        exp_ans (str) : expected answer\n",
    "        pred_ans (str) : predicted answer\n",
    "    Returns:\n",
    "        str : \"VALID\" if the answers are the same, \"NO MATCH\" otherwise\n",
    "    \"\"\"\n",
    "    if pred_ans == NO_ANS:\n",
    "        return \"NO MATCH\"\n",
    "    if clean_text(exp_ans) != clean_text(pred_ans):\n",
    "        return \"NO MATCH\"\n",
    "    return \"VALID\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    clean a text by removing punctuation and (some) stopwords.\n",
    "    Args:\n",
    "        text (str) : text to clean\n",
    "    Returns:\n",
    "        str : cleaned text\n",
    "    \"\"\"\n",
    "    # TODO: improve\n",
    "    # TODO: add support to french language\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r\"\\b(a|an|the|in|our)\\b\", \" \", text)\n",
    "    return re.sub(\" +\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def raw_f1_score(a_gold, a_pred):\n",
    "    \"\"\"\n",
    "    compute the raw F1 score between two answers.\n",
    "    Args:\n",
    "        a_gold (str) : expected answer\n",
    "        a_pred (str) : predicted answer\n",
    "    Returns:\n",
    "        float : F1 score\n",
    "    \"\"\"\n",
    "    if a_pred == \"\":\n",
    "        return 0\n",
    "    gold_toks = clean_text(a_gold).split()\n",
    "    pred_toks = clean_text(a_pred).split()\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def non_personal(question, nlp):\n",
    "    \"\"\"\n",
    "    check if a question contains personal pronouns.\n",
    "    Args:\n",
    "        question (str) : question to check\n",
    "        nlp (spacy.lang) : spacy language model\n",
    "    Returns:\n",
    "        bool : True if the question does not contain personal pronouns, False otherwise\n",
    "    \"\"\"\n",
    "    question_tok = nlp(question)\n",
    "    for tok in question_tok:\n",
    "        if tok.dep_ == \"nsubj\":\n",
    "            if (\n",
    "                tok.text.lower() == \"i\" or tok.text.lower() == \"you\"\n",
    "            ):  # TODO: add support to french language\n",
    "                return False\n",
    "        elif tok.dep_ == \"poss\":\n",
    "            if (\n",
    "                tok.text.lower() == \"my\" or tok.text.lower() == \"your\"\n",
    "            ):  # TODO: add support to french language\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# pylint:disable=invalid-name\n",
    "class MetadataExtractor:\n",
    "    def __init__(self):\n",
    "        self.metadata_extractor = MetafeatureExtractorsRunner()\n",
    "\n",
    "    def add_word_regex_matches_count(self, regex_rule, name=None):\n",
    "        \"\"\"\n",
    "        Adds a regex rule to the metadata extractor.\n",
    "        For a given regex return the number of words matching the regex.\n",
    "\n",
    "        Args:\n",
    "            regex_rule (str): regex rule to add\n",
    "        \"\"\"\n",
    "        self.metadata_extractor.add_metafeature_extractor(\n",
    "            WordRegexMatchesCount(regex=regex_rule, name=name)\n",
    "        )\n",
    "\n",
    "    def add_regex_match_count(self, regex_rule, name=None):\n",
    "        \"\"\"\n",
    "        Adds a regex rule to the metadata extractor.\n",
    "        For a given regex return the number of matches it has in the text.\n",
    "\n",
    "        Args:\n",
    "            regex_rule (str): regex rule to add\n",
    "        \"\"\"\n",
    "        self.metadata_extractor.add_metafeature_extractor(\n",
    "            RegexMatchCount(regex=regex_rule, name=name)\n",
    "        )\n",
    "\n",
    "    def compute(self, text):\n",
    "        \"\"\"\n",
    "        Computes metadata from a text using elemeta library and returns a dictionary of metadata.\n",
    "\n",
    "        Args:\n",
    "            text (str): text to extract metadata from\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary of metadata\n",
    "        \"\"\"\n",
    "        return self.metadata_extractor.run(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTScore:\n",
    "    def __init__(self, lan=\"en\", model_type=None):\n",
    "        \"\"\"\n",
    "        BERTScore computes a similarity score for each token in the candidate sentence with each\n",
    "        token in the reference sentence.\n",
    "        The final score is the average of the similarity scores of all tokens in the candidate sentence.\n",
    "\n",
    "        Args:\n",
    "            lan (str, optional): language to use. Defaults to \"en\", It may also be \"fr\". Depending\n",
    "            on the language, a different model is used by default.\n",
    "            model_type (sr, optional): Model to use. Defaults to None. If None, a default model is\n",
    "            used depending on the language (see above).\n",
    "        \"\"\"\n",
    "        if lan == \"fr\":\n",
    "            self.model_type = (\n",
    "                \"distilbert-base-multilingual-cased\" if not model_type else model_type\n",
    "            )  # TODO; find uncased version\n",
    "        elif lan == \"en\":\n",
    "            self.model_type = (\n",
    "                \"distilbert-base-uncased\" if not model_type else model_type\n",
    "            )\n",
    "        self.metric = load(\"bertscore\")\n",
    "\n",
    "    def compute(self, references, predictions, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            references (list): List of reference sentences.\n",
    "            predictions (list): List of candidate sentences.\n",
    "\n",
    "        Returns:\n",
    "            list: List of scores for each candidate sentence. Contains a list of scores for\n",
    "            precisions, recalls, and F1 scores.\n",
    "        \"\"\"\n",
    "        assert len(references) == len(\n",
    "            predictions\n",
    "        ), \"Number of references and predictions must be equal.\"\n",
    "        assert isinstance(references, list), \"References must be a list.\"\n",
    "        assert isinstance(predictions, list), \"Predictions must be a list.\"\n",
    "\n",
    "        return self.metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            model_type=self.model_type,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "class MAUVE:\n",
    "    def __init__(self, featurize_model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        MAUVE score computes the difference between the candidate sentence distribution\n",
    "        and the reference sentence distribution.\n",
    "        The bigger the MAUVE score, the better.\n",
    "        \"\"\"\n",
    "        self.metric = load(\"mauve\")\n",
    "        self.featurize_model_name = featurize_model_name\n",
    "\n",
    "    def compute(self, references, predictions, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            references (list): List of reference sentences.\n",
    "            predictions (list): List of candidate sentences.\n",
    "\n",
    "        Returns:\n",
    "            list: List of MAUVE scores for each candidate sentence.\n",
    "        \"\"\"\n",
    "        return self.metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            featurize_model_name=self.featurize_model_name,\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pylint:disable=too-many-locals\n",
    "class BLEURTScore:\n",
    "    def __init__(self, checkpoint=\"BLEURT-tiny\"):\n",
    "        \"\"\"\n",
    "        BLEURT is a learnt metric that uses BERT to compute a similarity score for each token\n",
    "        in the candidate sentence with each token in the reference sentence.\n",
    "\n",
    "        Args:\n",
    "            checkpoint (str, optional): Checkpoint to use. Defaults to BLEURT-tiny if not specified.\n",
    "        \"\"\"\n",
    "        self.checkpoint = checkpoint\n",
    "        self.metric = load(\"bleurt\", module_type=\"metric\", checkpoint=self.checkpoint)\n",
    "\n",
    "    def compute(self, references, predictions, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            references (list): List of reference sentences.\n",
    "            predictions (list): List of candidate sentences.\n",
    "\n",
    "        Returns:\n",
    "            list: List of scores for each candidate sentence.\n",
    "        \"\"\"\n",
    "        assert len(references) == len(\n",
    "            predictions\n",
    "        ), \"Number of references and predictions must be equal.\"\n",
    "        assert isinstance(references, list), \"References must be a list.\"\n",
    "        assert isinstance(predictions, list), \"Predictions must be a list.\"\n",
    "\n",
    "        return self.metric.compute(\n",
    "            predictions=predictions, references=references, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "class QSquared:\n",
    "    def __init__(self, lan=\"en\") -> None:\n",
    "        \"\"\"\n",
    "        Q² is a reference-free metric that aims to evaluate the factual consistency of knowledge-grounded\n",
    "        dialogue systems. The approach is based on automatic question generation and question answering\n",
    "        Source: https://github.com/orhonovich/q-squared\n",
    "\n",
    "        Args:\n",
    "            lan (str, optional): Language to use. Defaults to \"en\", It may also be \"fr\".\n",
    "        \"\"\"\n",
    "        self.qa_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
    "        )\n",
    "        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
    "        )\n",
    "        self.qg_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "        )\n",
    "        self.qg_model = AutoModelWithLMHead.from_pretrained(\n",
    "            \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "        )\n",
    "        assert lan in [\"fr\", \"en\"], \"Language must be either fr or en\"\n",
    "        self.bert_score = BERTScore(lan=lan)\n",
    "\n",
    "        if lan == \"fr\":\n",
    "            self.nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        elif lan == \"en\":\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def get_answer(\n",
    "        self, question: str, text: str\n",
    "    ):  # Code taken from https://huggingface.co/transformers/task_summary.html\n",
    "        \"\"\"\n",
    "        Search for the answer in the text given the question.\n",
    "        Args:\n",
    "            question (str) : question to ask\n",
    "            text (str) : text to search in\n",
    "        Returns:\n",
    "            answer (str) : answer to the question\n",
    "        \"\"\"\n",
    "        inputs = self.qa_tokenizer.encode_plus(\n",
    "            question, text, add_special_tokens=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        answer_start_scores, answer_end_scores = self.qa_model(\n",
    "            **inputs, return_dict=False\n",
    "        )\n",
    "\n",
    "        answer_start = torch.argmax(\n",
    "            answer_start_scores\n",
    "        )  # Get the most likely beginning of answer with the argmax of the score\n",
    "        answer_end = (\n",
    "            torch.argmax(answer_end_scores) + 1\n",
    "        )  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "        ans = self.qa_tokenizer.convert_tokens_to_string(\n",
    "            self.qa_tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "        )\n",
    "        return ans\n",
    "\n",
    "    def get_answer_candidates(self, text: str):\n",
    "        \"\"\"\n",
    "        Look for candidate aswers that could be answered by the text.\n",
    "        Args:\n",
    "            text (str) : text to search in\n",
    "        Returns:\n",
    "            candidates (str) : candidates answers\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        candidates = [ent.text for ent in list(doc.ents)]\n",
    "        noun_chunks = list(doc.noun_chunks)\n",
    "        for chunk in noun_chunks:\n",
    "            found = False\n",
    "            for cand in candidates:\n",
    "                if chunk.text.lower() == cand.lower():\n",
    "                    found = True\n",
    "            if not found:\n",
    "                candidates.append(chunk.text)\n",
    "        # candidates += [chunk.text for chunk in list(doc.noun_chunks) if chunk.text not in candidates]\n",
    "        candidates = [cand for cand in candidates if cand.lower() != \"i\"]\n",
    "        return candidates\n",
    "\n",
    "    def get_questions_beam(\n",
    "        self, answer, context, max_length=128, beam_size=5, num_return=5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get the n best questions for a given answer, given the context. \"Beam\" is the name of the\n",
    "        approach\n",
    "        Args:\n",
    "            answer (str) : answer to the question\n",
    "            context (str) : context to search in\n",
    "            max_length (int, optional) : max length of the generated question. Defaults to 128.\n",
    "            beam_size (int, optional) : beam size. Defaults to 5.\n",
    "            num_return (int, optional) : number of questions to return. Defaults to 5.\n",
    "        Returns:\n",
    "            all_questions (list) : n best questions\n",
    "        \"\"\"\n",
    "        all_questions = []\n",
    "        input_text = f\"answer: {answer}  context: {context} </s>\"\n",
    "        features = self.qg_tokenizer([input_text], return_tensors=\"pt\")\n",
    "\n",
    "        beam_outputs = self.qg_model.generate(\n",
    "            input_ids=features[\"input_ids\"],\n",
    "            attention_mask=features[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=beam_size,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_return_sequences=num_return,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "        for beam_output in beam_outputs:\n",
    "            all_questions.append(\n",
    "                self.qg_tokenizer.decode(beam_output, skip_special_tokens=True).replace(\n",
    "                    \"question: \", \"\", 1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return all_questions\n",
    "\n",
    "    def single_question_score(self, question, answer, response, knowledge):\n",
    "        \"\"\"\n",
    "        Given a candidate pair of question and answer (generated from the candidate text), get the\n",
    "        score of the aswer given by taking as a context the knowledge that the LLM was given.\n",
    "        The higher the F1-score, the more the model we are trying to evaluate is consistent\n",
    "        with the knowledge.\n",
    "        Args:\n",
    "            question (str) : cadidate question (generated from the candidate text)\n",
    "            answer (str) : candidate answer (generated from the candidate text)\n",
    "            response (str) : text generated by the LLM\n",
    "            knowledge (str) : knowledge given as a context to the LLM\n",
    "\n",
    "        Returns:\n",
    "            score, answer (tuple) : bert-score of the knowledge answer, knowledge answer\n",
    "        \"\"\"\n",
    "\n",
    "        pred_ans = self.get_answer(question, response)\n",
    "\n",
    "        if (\n",
    "            filter_questions(answer, pred_ans) == \"VALID\"\n",
    "        ):  # check if the answer is valid\n",
    "            knowledge_ans = self.get_answer(question, knowledge)\n",
    "            if knowledge_ans != NO_ANS:\n",
    "                score = self.bert_score.compute(\n",
    "                    references=[answer], predictions=[knowledge_ans]\n",
    "                )\n",
    "                return score[\"f1\"][0], knowledge_ans\n",
    "            return 0, NO_ANS\n",
    "        return INVALID_QUESTION, INVALID_QUESTION\n",
    "\n",
    "    def compute(self, response, knowledge, single=False, remove_personal=True):\n",
    "        \"\"\"\n",
    "        Compute the Q² score for a given response and knowledge.\n",
    "        Args:\n",
    "            response (str) : text generated by the LLM\n",
    "            knowledge (str) : knowledge given as a context to the LLM\n",
    "            single (bool) : if True, only one question is generated for each candidate answer.\n",
    "                            Defaults to False.\n",
    "            remove_personal (bool) : if True, remove questions that contain personal pronouns.\n",
    "                                     Defaults to True.\n",
    "        Returns:\n",
    "            avg_f1 (float) : average F1-bert-score of the knowledge answers (Q² score)\n",
    "        \"\"\"\n",
    "\n",
    "        f1_bert_score = 0\n",
    "        num_questions = 0\n",
    "\n",
    "        # valid_questions = []\n",
    "        # valid_cands = []\n",
    "        # knowledge_answers = []\n",
    "        # scores = []\n",
    "\n",
    "        candidates = self.get_answer_candidates(response)\n",
    "        for cand in candidates:\n",
    "            questions = self.get_questions_beam(cand, response)\n",
    "            for question in questions:\n",
    "                if not remove_personal or non_personal(question, self.nlp):\n",
    "                    question_score, _ = self.single_question_score(\n",
    "                        question, cand, response, knowledge\n",
    "                    )\n",
    "                    if question_score != INVALID_QUESTION:\n",
    "                        num_questions += 1\n",
    "                        f1_bert_score += question_score\n",
    "\n",
    "                        # valid_questions.append(question)\n",
    "                        # valid_cands.append(cand)\n",
    "                        # knowledge_answers.append(knowledge_ans)\n",
    "                        # scores.append(question_score)\n",
    "\n",
    "                        if single:\n",
    "                            break\n",
    "\n",
    "        if num_questions:\n",
    "            avg_f1 = f1_bert_score / num_questions\n",
    "        else:\n",
    "            avg_f1 = INVALID_QUESTION\n",
    "        return avg_f1  # , valid_questions, valid_cands, knowledge_answers, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfCheckGPT:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        eval_model_name_or_path=\"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
    "        eval_model_basename=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This class implements the self-check GPT evaluation metric for generative language models.\n",
    "        It is inspired by the self-check metric proposed in https://arxiv.org/pdf/2303.08896.pdf.\n",
    "        Args:\n",
    "            model (transformers.PreTrainedModel): GPT model to evaluate.\n",
    "            eval_model_name_or_path (str): Evaluation model name or path. Defaults to \"TheBloke/Llama-2-7b-Chat-GGUF\".\n",
    "            eval_model_basename (str): Evaluation model basename. Defaults to \"llama-2-7b-chat.Q4_K_M.gguf\".\n",
    "        \"\"\"\n",
    "        assert isinstance(\n",
    "            eval_model_name_or_path, str\n",
    "        ), \"eval_model_name_or_path must be a string.\"\n",
    "        assert isinstance(\n",
    "            eval_model_basename, str\n",
    "        ), \"eval_model_basename must be a string.\"\n",
    "\n",
    "        self.model = model\n",
    "        self.eval_model_path = hf_hub_download(\n",
    "            repo_id=eval_model_name_or_path, filename=eval_model_basename\n",
    "        )\n",
    "\n",
    "        self.eval_model = Llama(\n",
    "            model_path=self.eval_model_path, n_threads=2, verbose=False  # CPU cores\n",
    "        )\n",
    "\n",
    "    def get_prompt(self, pred, sample, question):\n",
    "        \"\"\"\n",
    "        This method returns a prompt template given a candidate sentence, a sample sentence, and a question.\n",
    "        Args:\n",
    "            pred (str): Candidate sentence.\n",
    "            sample (str): Sample sentence.\n",
    "            question (str): Question asked to the model for which it generated $pred.\n",
    "\n",
    "        Returns:\n",
    "            str: Prompt template.\n",
    "        \"\"\"\n",
    "        system_prompt = \"You are a helpful, polite and concise assistant. Your task is to check if two texts provide the same answer to a given question. Always answer with a single word. The possible answers are either YES or NO.\\n\\n\"\n",
    "        question = \"###Question:\\n\" + question\n",
    "        text1 = \"\\n###Text 1: \" + sample\n",
    "        text2 = \"\\n###Text 2: \" + pred\n",
    "\n",
    "        prompt_template = f\"\"\"SYSTEM: {system_prompt}\n",
    "        USER: {question + text1 + text2}\n",
    "        ASSISTANT (YES or NO):\"\"\"\n",
    "\n",
    "        return prompt_template\n",
    "\n",
    "    def get_prompts(self, pred, samples, question):\n",
    "        \"\"\"\n",
    "        This method returns a list of prompt templates given a candidate sentence, a list\n",
    "        of sample sentences, and a question.\n",
    "        Args:\n",
    "            pred (str): Candidate sentence.\n",
    "            samples (list of str): List of sample sentences.\n",
    "            question (str): Question asked to the model for which it generated $pred.\n",
    "\n",
    "        Returns:\n",
    "            list: List of prompt templates.\n",
    "        \"\"\"\n",
    "        print(samples)\n",
    "        return [self.get_prompt(pred, sample, question) for sample in samples]\n",
    "\n",
    "    def compute(self, question, pred, n_samples):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            question (str): Question asked to the model for which it generated $pred.\n",
    "            pred (str): Candidate sentence.\n",
    "            n_samples (int): Number of samples to generate.\n",
    "\n",
    "        Returns:\n",
    "            score (float): Score for the candidate sentence.\n",
    "        \"\"\"\n",
    "        assert isinstance(question, str), \"Prediction must be a string.\"\n",
    "        assert isinstance(pred, str), \"Prediction must be a string.\"\n",
    "        assert isinstance(n_samples, int), \"Number of samples must be an integer.\"\n",
    "        assert n_samples > 0, \"Number of samples must be greater than 0.\"\n",
    "        assert question and pred, \"Question and prediction must be non-empty.\"\n",
    "\n",
    "        # Generate n_samples samples from the model\n",
    "        samples = []\n",
    "        print(\"Samples:\\n\")\n",
    "        for _ in range(n_samples):\n",
    "            system_prompt = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.\"\n",
    "            prompt_template = f\"\"\"SYSTEM: {system_prompt}\n",
    "            USER: {question}\n",
    "            ASSISTANT:\"\"\"\n",
    "\n",
    "            response = self.model(prompt_template, max_tokens=200)\n",
    "            sample = response[\"choices\"][0][\"text\"]\n",
    "            print(sample, \"\\n\")\n",
    "            samples.append(sample)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # For each sample, ask evaluator model to evaluate the sample\n",
    "        prompts = self.get_prompts(pred, samples, question)\n",
    "        scores = []\n",
    "        print(\"Prompts:\\n\")\n",
    "        for prompt in prompts:\n",
    "            print(prompt, \"\\n\")\n",
    "            answer = self.eval_model(prompt, max_tokens=200)[\"choices\"][0][\"text\"]\n",
    "            print(answer, \"\\n\")\n",
    "            scores.append(answer)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Compute the score: how often the sentence if supported by the sample\n",
    "        score = np.mean([1 if \"yes\" in score.lower() else 0 for score in scores])\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "class GEval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path=\"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
    "        model_basename=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This class implements the GEval evaluation metric for generative language models.\n",
    "        It is inspired by the GEval metric proposed in https://arxiv.org/pdf/2303.16634.pdf.\n",
    "        Args:\n",
    "            model_name_or_path (str): Model name or path. Defaults to \"TheBloke/Llama-2-7b-Chat-GGUF\".\n",
    "            model_basename (str): Model basename. Defaults to \"llama-2-7b-chat.Q4_K_M.gguf\".\n",
    "        \"\"\"\n",
    "        assert isinstance(\n",
    "            model_name_or_path, str\n",
    "        ), \"model_name_or_path must be a string.\"\n",
    "        assert isinstance(model_basename, str), \"model_basename must be a string.\"\n",
    "\n",
    "        self.model_path = hf_hub_download(\n",
    "            repo_id=model_name_or_path, filename=model_basename\n",
    "        )\n",
    "\n",
    "        self.lcpp_llm = Llama(\n",
    "            model_path=self.model_path,\n",
    "            n_threads=2,  # CPU cores\n",
    "            logits_all=True,\n",
    "            n_ctx=1000,\n",
    "        )\n",
    "\n",
    "        self.tasks = {\n",
    "            \"summ\": \"You will be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\",\n",
    "            \"diag\": \"You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well. Your task is to rate the responses on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\",\n",
    "        }\n",
    "        self.aspects = {\n",
    "            \"COH\": {\n",
    "                \"name\": \"Coherence\",\n",
    "                \"prompt\": \"Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby ”the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.”\",\n",
    "            },\n",
    "            \"CON\": {\n",
    "                \"name\": \"Consistency\",\n",
    "                \"prompt\": \"Consistency (1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts. \",\n",
    "            },\n",
    "            \"ENG\": {\n",
    "                \"name\": \"Engagingness\",\n",
    "                \"prompt\": \"Engagingness (1-5) - Is the response dull/interesting? - A score of 1 indicates that the response is dull and uninteresting. A score of 5 indicates that the response is interesting and engaging.\",\n",
    "            },\n",
    "            \"FLU\": {\n",
    "                \"name\": \"Fluency\",\n",
    "                \"prompt\": \"Fluency (1-5) - the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure. - 1: Poor. The summary is difficult to read and understand. It contains many grammatical errors, spelling mistakes, and/or punctuation errors. - 2: Fair. The summary is somewhat difficult to read and understand. It contains some grammatical errors, spelling mistakes, and/or punctuation errors. - 3: Good. The summary is easy to read and understand. It contains few grammatical errors, spelling mistakes, and/or punctuation errors. - 4: Very Good. The summary is easy to read and understand. It contains no grammatical errors, spelling mistakes, and/or punctuation errors. - 5: Excellent. The summary is easy to read and understand. It contains no grammatical errors, spelling mistakes, and/or punctuation errors.\",\n",
    "            },\n",
    "            \"REL\": {\n",
    "                \"name\": \"Relevance\",\n",
    "                \"prompt\": \"Relevance (1-5) - selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.\",\n",
    "            },\n",
    "            \"POL\": {\n",
    "                \"name\": \"Politeness\",\n",
    "                \"prompt\": \"Politeness (1-5) - the degree to which the response is polite. - 1: Very impolite. The response is very impolite. - 2: Somewhat impolite. The response is somewhat impolite. - 3: Neutral. The response is neutral. - 4: Somewhat polite. The response is somewhat polite. - 5: Very polite. The response is very polite.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def get_prediction(self, prompt):\n",
    "        \"\"\"\n",
    "        This method returns a prediction given a prompt template.\n",
    "        Args:\n",
    "            prompt (str): Prompt template.\n",
    "\n",
    "        Returns:\n",
    "            response (dict): Response from the model.\n",
    "        \"\"\"\n",
    "        response = self.lcpp_llm.create_completion(\n",
    "            prompt=prompt,\n",
    "            max_tokens=250,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            logprobs=5,\n",
    "            repeat_penalty=1.2,\n",
    "            top_k=50,\n",
    "            echo=True,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def get_cot(self, prompt):\n",
    "        \"\"\"\n",
    "        This method returns a chain of thoughts given a prompt template.\n",
    "        Args:\n",
    "            prompt (str): Prompt template.\n",
    "\n",
    "        Returns:\n",
    "            cot (str): Chain of thoughts.\n",
    "        \"\"\"\n",
    "        title = \"\\nEvaluation steps:\\n\"\n",
    "        cot = self.get_prediction(prompt + title)[\"choices\"][0][\"text\"]\n",
    "        return cot\n",
    "\n",
    "    # pylint: disable=consider-iterating-dictionary\n",
    "    def get_prompt(self, src, pred, task, aspect, custom_prompt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src (str): Source text.\n",
    "            pred (str): Candidate sentence to evaluate.\n",
    "            task (str): Definition of the task.\n",
    "            aspect (str): Evaluation criterion code.\n",
    "            custom_prompt (dict): Custom prompt template.\n",
    "                Must contain the following keys: \"task\", \"aspect\", \"name\".\n",
    "        \"\"\"\n",
    "        definition = (\n",
    "            \"\\n Task definition:\\n\" + self.tasks[task]\n",
    "            if task in self.tasks.keys()\n",
    "            else custom_prompt[\"task\"]\n",
    "        )\n",
    "        crit = (\n",
    "            \"\\n Evaluation criteria:\\n\" + self.aspects[aspect][\"prompt\"]\n",
    "            if aspect in self.aspects.keys()\n",
    "            else custom_prompt[\"aspect\"]\n",
    "        )\n",
    "        name = (\n",
    "            self.aspects[aspect][\"name\"]\n",
    "            if aspect in self.aspects.keys()\n",
    "            else custom_prompt[\"name\"]\n",
    "        )\n",
    "\n",
    "        prompt = f\"{definition} {crit}\"\n",
    "\n",
    "        # Chain of thoughts, set of intermediate instructions generated by llm detailing evaluation steps\n",
    "        auto_cot = self.get_cot(prompt)\n",
    "\n",
    "        return (\n",
    "            prompt\n",
    "            + auto_cot\n",
    "            + \"\\n Example:\\n Source Text:\\n\"\n",
    "            + src\n",
    "            + \"\\n Generated text:\\n\"\n",
    "            + pred\n",
    "            + \"\\n Evaluation Form (scores ONLY):\\n\"\n",
    "            + name\n",
    "            + \": \"\n",
    "        )\n",
    "\n",
    "    def get_score(self, prompt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prompt (str): Prompt template.\n",
    "\n",
    "        Returns:\n",
    "            score (float): Score for the candidate sentence.\n",
    "        \"\"\"\n",
    "        response = self.get_prediction(prompt)\n",
    "        tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
    "        top_logprobs = response[\"choices\"][0][\"logprobs\"][\"top_logprobs\"]\n",
    "\n",
    "        # Extract evaluation form from tokens ()\n",
    "        template_tokens = [\n",
    "            \" E\",\n",
    "            \"valu\",\n",
    "            \"ation\",\n",
    "            \" Form\",\n",
    "            \" (\",\n",
    "            \"sc\",\n",
    "            \"ores\",\n",
    "            \" ON\",\n",
    "            \"LY\",\n",
    "            \"):\",\n",
    "        ]\n",
    "        start_index = tokens.index(template_tokens[-1]) + 1\n",
    "        # Extract number index from the remaining tokens\n",
    "        for token in tokens[start_index:]:\n",
    "            if token.isdigit():\n",
    "                number_index = tokens.index(token)\n",
    "                break\n",
    "\n",
    "        # Get logprobs associated with number\n",
    "        logprobs = top_logprobs[number_index]\n",
    "\n",
    "        # Compute score\n",
    "        # Get only keys that are numbers\n",
    "        number_keys = [int(key) for key in logprobs.keys() if key.isdigit()]\n",
    "        number_logprobs = [logprobs[str(key)] for key in number_keys]\n",
    "        number_probs = [np.exp(logprob) for logprob in number_logprobs]\n",
    "\n",
    "        score = np.sum(np.multiply(number_keys, number_probs)) / len(number_keys)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def compute(self, source, pred, task, aspect, custom_prompt=None):\n",
    "        \"\"\"\n",
    "        This method computes the GEval score for a candidate sentence given a source text,\n",
    "        a prompt template, an aspect to evaluate, and a task description.\n",
    "        Args:\n",
    "            source (str): Source text.\n",
    "            pred (str): Candidate sentence to evaluate.\n",
    "            task (str): Definition of the task.\n",
    "            aspect (str): Evaluation criterion code.\n",
    "            custom_prompt (str, optional): Custom prompt template. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            score (float): Score for the candidate sentence.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, str), \"Source must be a string.\"\n",
    "        assert isinstance(pred, str), \"Pred must be a string.\"\n",
    "        assert isinstance(task, str), \"Definition must be a string.\"\n",
    "        assert isinstance(aspect, str), \"Criterion must be a string.\"\n",
    "        assert custom_prompt is None or isinstance(\n",
    "            custom_prompt, str\n",
    "        ), \"Criterion name must be a string.\"\n",
    "        assert (\n",
    "            aspect in self.aspects.keys() or custom_prompt is not None\n",
    "        ), \"Criterion name must be given if criterion is not in the list of criteria.\"\n",
    "        if not custom_prompt:\n",
    "            assert task and aspect, \"Task and aspect must be given if no custom prompt is given.\"\n",
    "        if not (task and aspect):\n",
    "            assert custom_prompt, \"A custom prompt must be given if task and aspect are not given.\"\n",
    "\n",
    "\n",
    "        prompt = self.get_prompt(source, pred, task, aspect, custom_prompt)\n",
    "        return self.get_score(prompt)\n",
    "\n",
    "\n",
    "class GPTScore:\n",
    "    # pylint: disable=f-string-without-interpolation\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path=\"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
    "        model_basename=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This class implements the GPTScore evaluation metric for generative language models.\n",
    "        It is inspired by the GPTScore metric proposed in https://arxiv.org/pdf/2302.04166.pdf.\n",
    "        Args:\n",
    "            model_name_or_path (str): Model name or path. Defaults to \"TheBloke/Llama-2-7b-Chat-GGUF\".\n",
    "            model_basename (str): Model basename. Defaults to \"llama-2-7b-chat.Q4_K_M.gguf\".\n",
    "        \"\"\"\n",
    "        assert isinstance(\n",
    "            model_name_or_path, str\n",
    "        ), \"model_name_or_path must be a string.\"\n",
    "        assert isinstance(model_basename, str), \"model_basename must be a string.\"\n",
    "\n",
    "        self.templates = {\n",
    "            \"summ\": {\n",
    "                \"FAC\": f\"Generate a summary with consistent facts for the following text: {{src}}\\n\\nTl;dr{{pred}}\",\n",
    "                \"COV\": f\"Generate a summary with as much semantic coverage as possible for the following text: {{src}}\\n\\nTl;dr{{pred}}\",\n",
    "                \"CON\": f\"Generate factually consistent summary for the following text: {{src}}\\n\\nTl;dr{{pred}}\",\n",
    "                \"INF\": f\"Generate an informative summary that captures the key points of the following text:{{src}}\\n\\nTl;dr{{pred}}\",\n",
    "                \"COH\": f\"Generate a coherent summary for the following text: {{src}}\\n\\nTl;dr{{pred}}\",\n",
    "                \"REL\": f\"Generate a relevant summary with consistent details for the following text: {{src}}\\n\\nTl;dr{{pred}}\",\n",
    "                \"FLU\": f\"Generate a fluent and grammatical summary for the following text: {{src}}\\n\\nTl;dr{{pred}}\",\n",
    "            },\n",
    "            \"MT\": {\n",
    "                \"ACC\": f\"Rewrite the following text with its core information and consistent facts:{{src}} In other words, {{pred}}\",\n",
    "                \"FLU\": f\"Rewrite the following text to make it more grammatical and well-written:{{src}} In other words,{{pred}}\",\n",
    "                \"MQM\": f\"Rewrite the following text into high-quality text with its core information:{{src}} In other words,{{pred}}\",\n",
    "            },\n",
    "            \"D2T\": {\n",
    "                \"INF\": f\"Convert the following text to another expression that preserves key information:\\n\\n{{src}} In other words, {{pred}}\",\n",
    "                \"NAT\": f\"Convert the following text into another expression that is human-like and natural:\\n\\n{{src}} In other words, {{pred}}\",\n",
    "                \"FLU\": f\"Convert the following text into another expression that preserves key information and is human-like and natural:\\n\\n{{src}} In other words, {{pred}}\",\n",
    "            },\n",
    "            \"diag\": {\n",
    "                \"COH\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI coherent and maintains a good conversation flow throughout the conversation? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"DIV\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Is there diversity in the AI responses? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"FLE\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI flexible and adaptable to human and their interests? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"UND\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI seem to understand the human? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"INQ\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI inquisitive throughout the conversation? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"CON\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI consistent in the information it provides throughout the conversation? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"INF\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI informative throughout the conversation? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"LIK\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI display a likeable personality? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"DEP\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI discuss topics in depth? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "                \"ERR\": f\"Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI able to recover from errors that it makes? (a) Yes. (b) No.\\nConversation:\\nUser: {{src}}\\nAI: {{pred}}\\nAnswer:\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.tasks = self.templates.keys()\n",
    "        self.aspects = list(\n",
    "            {aspect for task in self.tasks for aspect in self.templates[task]}\n",
    "        )\n",
    "\n",
    "        self.model_path = hf_hub_download(\n",
    "            repo_id=model_name_or_path, filename=model_basename\n",
    "        )\n",
    "\n",
    "        self.lcpp_llm = Llama(\n",
    "            model_path=self.model_path,\n",
    "            n_threads=2,  # CPU cores\n",
    "            logits_all=True,\n",
    "        )\n",
    "\n",
    "    def get_prompts(self, aspect, task, sources, preds):\n",
    "        \"\"\"\n",
    "        This method returns a list of prompt templates given a task description, and an aspect to evaluate.\n",
    "        Args:\n",
    "            aspect (str): Aspect to evaluate.\n",
    "            task (str): Task description.\n",
    "            sources (list of str): Source texts.\n",
    "            preds (list of str): Candidate sentences.\n",
    "        Returns:\n",
    "            list: List of prompt templates.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.get_prompt(aspect, task, src, pred)\n",
    "            for (src, pred) in zip(sources, preds)\n",
    "        ]\n",
    "\n",
    "    def get_prompt(self, aspect, task, src, pred):\n",
    "        \"\"\"\n",
    "        This method returns a prompt template given a task description, and an aspect to evaluate.\n",
    "        Args:\n",
    "            aspect (str): Aspect to evaluate.\n",
    "            task (str): Task description.\n",
    "            src (str): Source text.\n",
    "            pred (str): Candidate sentence.\n",
    "        Returns:\n",
    "            str: Prompt template.\n",
    "        \"\"\"\n",
    "        # Check that the corresponding entry exists in the prompt template\n",
    "        assert (\n",
    "            aspect in self.templates[task]\n",
    "        ), f\"Aspect {aspect} is not available for task {task}.\"\n",
    "        # Check that the prompt template is not empty\n",
    "        assert self.templates[task][\n",
    "            aspect\n",
    "        ], f\"Prompt template for aspect {aspect} and task {task} is non-existent. Please specify a prompt template.\"\n",
    "\n",
    "        template = self.templates[task][aspect]\n",
    "\n",
    "        # Replace placeholders with source and candidate sentence\n",
    "        template = template.replace(\"{src}\", src)\n",
    "        template = template.replace(\"{pred}\", pred)\n",
    "\n",
    "        return template\n",
    "\n",
    "    def compute(self, source, pred, prompt=None, aspect=None, task=None):\n",
    "        \"\"\"\n",
    "        This method computes the GPTScore for a candidate sentence given a source text,\n",
    "        a prompt template, an aspect to evaluate, and a task description.\n",
    "        Args:\n",
    "            source (str): Source text.\n",
    "            pred (str): Candidate sentence.\n",
    "            prompt (str, optional): Prompt template. Defaults to None.\n",
    "            aspect (str, optional): Aspect to evaluate. Defaults to None.\n",
    "            task (str, optional): Task description. Defaults to None.\n",
    "        Returns:\n",
    "            score (float): Score for the candidate sentence.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, str), \"Source must be a string.\"\n",
    "        assert isinstance(pred, str), \"Pred must be a string.\"\n",
    "\n",
    "        # If prompt is given, check that it is a string\n",
    "        if prompt:\n",
    "            assert isinstance(prompt, str), \"Prompt must be a string.\"\n",
    "            assert not aspect, \"Aspect must not be given if prompt is given.\"\n",
    "            assert not task, \"Task must not be given if prompt is given.\"\n",
    "        else:\n",
    "            # If prompt is not given, check that task and aspect are given\n",
    "            assert aspect, \"Aspect must be given if prompt is not given.\"\n",
    "            assert task, \"Task must be given if prompt is not given.\"\n",
    "\n",
    "        # If aspect is given, check that it is a string\n",
    "        if aspect:\n",
    "            assert isinstance(aspect, str), \"Aspect must be a string.\"\n",
    "            assert aspect in self.aspects, f\"Aspect must be one of {self.aspects}.\"\n",
    "\n",
    "        # If task is given, check that it is a string\n",
    "        if task:\n",
    "            assert isinstance(task, str), \"Task must be a string.\"\n",
    "            assert task in self.tasks, f\"Task must be one of {self.tasks}.\"\n",
    "\n",
    "        # Generative LLM is given a prompt template and some context information\n",
    "        if not prompt:\n",
    "            prompt = self.get_prompt(aspect, task, source, pred)\n",
    "\n",
    "        response = self.lcpp_llm.create_completion(\n",
    "            prompt=prompt,\n",
    "            max_tokens=500,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            logprobs=1,\n",
    "            repeat_penalty=1.2,\n",
    "            top_k=50,\n",
    "            echo=True,\n",
    "        )\n",
    "\n",
    "        # Compute logprobs\n",
    "        # Find the end position of the input...\n",
    "        print(response[\"choices\"][0][\"logprobs\"][\"text_offset\"])\n",
    "        i = response[\"choices\"][0][\"logprobs\"][\"text_offset\"].index(len(prompt))\n",
    "        if i == 0:\n",
    "            i = i + 1\n",
    "\n",
    "        # Get logprobs\n",
    "        loss = -sum(\n",
    "            response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][i:-1]\n",
    "        )  # ignore the last '.'\n",
    "        avg_loss = loss / (\n",
    "            len(response[\"choices\"][0][\"logprobs\"][\"text_offset\"]) - i - 1\n",
    "        )  # 1 is the last '.'\n",
    "\n",
    "        return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        lan=\"en\",\n",
    "        bleurt_model=\"BLEURT-tiny\",\n",
    "        mauve_model=\"gpt2\",\n",
    "        eval_model_name_or_path=\"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
    "        eval_model_basename=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "        model_name_or_path=\"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
    "        model_basename=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    ) -> None:\n",
    "        assert isinstance(lan, str), \"lan must be a string.\"\n",
    "        assert isinstance(bleurt_model, str), \"bleurt_model must be a string.\"\n",
    "        assert isinstance(mauve_model, str), \"mauve_model must be a string.\"\n",
    "        assert isinstance(eval_model_name_or_path, str), (\n",
    "            \"eval_model_name_or_path must be a string.\"\n",
    "        )\n",
    "        assert isinstance(eval_model_basename, str), (\n",
    "            \"eval_model_basename must be a string.\"\n",
    "        )\n",
    "        assert isinstance(model_name_or_path, str), (\n",
    "            \"model_name_or_path must be a string.\"\n",
    "        )\n",
    "        assert isinstance(model_basename, str), \"model_basename must be a string.\"\n",
    "\n",
    "        # Metrics\n",
    "        self.bert_score = BERTScore(lan=lan)\n",
    "        self.mauve = MAUVE(featurize_model_name=mauve_model)\n",
    "        self.bleurt_score = BLEURTScore(checkpoint=bleurt_model)\n",
    "        self.q_squared = QSquared(lan=lan)\n",
    "        self.selfcheckgpt = SelfCheckGPT(\n",
    "            model,\n",
    "            eval_model_name_or_path=eval_model_name_or_path,\n",
    "            eval_model_basename=eval_model_basename,\n",
    "        )\n",
    "        self.geval = GEval(\n",
    "            model_name_or_path=model_name_or_path, model_basename=model_basename\n",
    "        )\n",
    "        self.gptscore = GPTScore(\n",
    "            model_name_or_path=model_name_or_path, model_basename=model_basename\n",
    "        )\n",
    "\n",
    "        # Metadata\n",
    "        self.metadata_extractor = MetadataExtractor()\n",
    "\n",
    "    def score(\n",
    "        self,\n",
    "        input: str,\n",
    "        prompt: str,\n",
    "        prediction: str,\n",
    "        context: str = None,\n",
    "        reference: str = None,\n",
    "        n_samples: int = 5,\n",
    "        task: str = None,\n",
    "        aspects: list = None,\n",
    "        custom_prompt: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (str): Input to the model.\n",
    "            prompt (str): Prompt to the model. Comprises the context and the input.\n",
    "            prediction (str): Prediction of the model.\n",
    "            context (str, optional): Context of the prediction. Defaults to None.\n",
    "            reference (str, optional): Reference of the prediction. Defaults to None.\n",
    "            n_samples (int, optional): Number of samples to generate. Defaults to 5.\n",
    "            task (str, optional): Task definition. Defaults to None.\n",
    "            aspects (list, optional): Aspects to evaluate. Defaults to None.\n",
    "            custom_prompt (str, optional): Custom prompt. Defaults to None.\n",
    "        \"\"\"\n",
    "        assert isinstance(prompt, str), \"prompt must be a string.\"\n",
    "        assert isinstance(input, str), \"input must be a string.\"\n",
    "        assert isinstance(context, str), \"context must be a string.\"\n",
    "        assert isinstance(prediction, str), \"prediction must be a string.\"\n",
    "        assert (\n",
    "            isinstance(reference, str) or reference is None\n",
    "        ), \"Reference must be a string or None.\"\n",
    "        assert isinstance(n_samples, int), \"n_samples must be an integer.\"\n",
    "        assert n_samples > 0, \"n_samples must be greater than 0.\"\n",
    "        assert (\n",
    "            isinstance(task, str) or task is None\n",
    "        ), \"task must be a string or None.\"\n",
    "        assert (\n",
    "            isinstance(aspects, list) or aspects is None\n",
    "        ), \"aspects must be a list or None.\"\n",
    "        assert (\n",
    "            isinstance(custom_prompt, str) or custom_prompt is None\n",
    "        ), \"custom_prompt must be a string or None.\"\n",
    "\n",
    "\n",
    "        if aspects:\n",
    "            geval_scores = {key: 0 for key in task}\n",
    "            gpt_scores = {key: 0 for key in task}\n",
    "            for aspect in aspects:\n",
    "                geval_scores[aspect] = self.geval.compute(\n",
    "                    prompt, prediction, task, aspect, custom_prompt\n",
    "                )\n",
    "                gpt_scores[aspect] = self.gptscore.compute(\n",
    "                    prompt, prediction, custom_prompt, aspect, task\n",
    "                )\n",
    "\n",
    "        metadata_dict = {\n",
    "            \"prompt\": self.metadata_extractor.compute(prompt),\n",
    "            \"input\": self.metadata_extractor.compute(input),\n",
    "            \"context\": self.metadata_extractor.compute(context),\n",
    "            \"prediction\": self.metadata_extractor.compute(prediction),\n",
    "            \"reference\": self.metadata_extractor.compute(reference)\n",
    "            if reference\n",
    "            else None,\n",
    "        }\n",
    "\n",
    "        metrics_dict = {\n",
    "            \"bert_score\": self.bert_score.compute([reference], [prediction])\n",
    "            if reference\n",
    "            else None,\n",
    "            \"mauve\": self.mauve.compute([reference], [prediction])\n",
    "            if reference\n",
    "            else None,\n",
    "            \"bleurt_score\": self.bleurt_score.compute([reference], [prediction])\n",
    "            if reference\n",
    "            else None,\n",
    "            \"q_squared\": self.q_squared.compute(prediction, context),\n",
    "            \"selfcheck_gpt\": self.selfcheckgpt.compute(prompt, prediction, n_samples),\n",
    "            \"g_eval\": self.geval.compute(prompt, prediction, custom_prompt) if custom_prompt else geval_scores if aspects and task else None,\n",
    "            \"gpt_score\": self.gptscore.compute(prompt, prediction, custom_prompt) if custom_prompt else gpt_scores if aspects and task else None,\n",
    "        }\n",
    "\n",
    "        output = {\n",
    "            \"metadata\": metadata_dict,\n",
    "            \"metrics\": metrics_dict,\n",
    "        }\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLLMScorer(unittest.TestCase):\n",
    "    def test_init(self):\n",
    "        model = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
    "        false = False\n",
    "        with self.assertRaises(AssertionError):\n",
    "            \n",
    "            LLMScorer(model=model, lan=false)\n",
    "            LLMScorer(model=model, bleurt_model=false)\n",
    "            LLMScorer(model=model, mauve_model=false)\n",
    "            LLMScorer(model=model, eval_model_name_or_path=false)\n",
    "            LLMScorer(model=model, eval_model_basename=false)\n",
    "            LLMScorer(model=model, model_name_or_path=false)\n",
    "            LLMScorer(model=model, model_basename=false)\n",
    "\n",
    "    def test_score_bad_arguments(self):\n",
    "        model = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
    "        scorer = LLMScorer(model=model)\n",
    "\n",
    "        input = \"I am a dog.\"\n",
    "        prompt = f\"System: You are a cat. You don't like dogs. User: {input}\"\n",
    "        context = \"System: You are a cat. You don't like dogs.\"\n",
    "        prediction = \"I am a cat, I don't like dogs.\"\n",
    "        reference = \"I am a cat, I don't like dogs, miau.\"\n",
    "        n_samples = 5\n",
    "        task = \"diag\"\n",
    "        aspect = [\"FLU\"]\n",
    "        criterion_name = \"Fluency\"\n",
    "        custom_prompt = \"System: You are an evaluator. You must evaluate the fluency of the following dialog.\"\n",
    "\n",
    "        with self.assertRaises(AssertionError):\n",
    "            scorer.score(False, prompt, context, prediction, reference)\n",
    "            scorer.score(input, False, context, prediction, reference)\n",
    "            scorer.score(input, prompt, False, prediction, reference)\n",
    "            scorer.score(input, prompt, context, False, reference)\n",
    "            scorer.score(input, prompt, context, prediction, False)\n",
    "            scorer.score(input, prompt, context, prediction, reference, n_samples=False)\n",
    "            scorer.score(input, prompt, context, prediction, reference, task=False)\n",
    "            scorer.score(input, prompt, context, prediction, reference, aspects=False)\n",
    "            scorer.score(input, prompt, context, prediction, reference, criterion_name=False)\n",
    "            scorer.score(input, prompt, context, prediction, reference, custom_prompt=False)\n",
    "\n",
    "    def test_score(self):\n",
    "        model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
    "        model_basename = \"llama-2-7b-chat.Q2_K.gguf\"  # the model is in bin format\n",
    "\n",
    "        model_path = hf_hub_download(\n",
    "            repo_id=model_name_or_path, filename=model_basename\n",
    "        )\n",
    "        model = Llama(model_path=model_path, n_threads=2, verbose=False)  # CPU cores\n",
    "\n",
    "        scorer = LLMScorer(model=model, eval_model_name_or_path=model_name_or_path, eval_model_basename=model_basename, model_name_or_path=model_name_or_path, model_basename=model_basename)\n",
    "\n",
    "        input = \"I am a dog.\"\n",
    "        prompt = f\"System: You are a cat. You don't like dogs. User: {input}\"\n",
    "        context = \"Examples: Eww, I hate dogs.\"\n",
    "        prediction = \"I am a cat, I don't like dogs.\"\n",
    "        reference = \"I am a cat, I don't like dogs, miau.\"\n",
    "        task = \"diag\"\n",
    "        aspect = [\"FLU\"]\n",
    "        custom_prompt = {\"name\": \"Fluency\", \"task\": \"Dialog\", \"aspect\": \"Evaluate the fluency of the following dialog.\"}\n",
    "\n",
    "        scores = scorer.score(input, prompt, context, prediction, reference)\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"scores\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # All default\n",
    "        print(\"All default\")\n",
    "        scores = scorer.score(\n",
    "            input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"scores\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # All default, but with context\n",
    "        print(\"All default, but with context\")\n",
    "        scores = scorer.score(\n",
    "            input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "            context=context\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"scores\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # All default, but with reference\n",
    "        print(\"All default, but with reference\")\n",
    "        scores = scorer.score(\n",
    "            input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "            reference=reference,\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"scores\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # Precise task and aspect\n",
    "        print(\"Precise task and aspect\")\n",
    "        scores = scorer.score(\n",
    "            input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "            task=task,\n",
    "            aspects=aspect,\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"scores\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # Precise custom prompt\n",
    "        print(\"Precise custom prompt\")\n",
    "        scores = scorer.score(\n",
    "            input,\n",
    "            prompt,\n",
    "            context,\n",
    "            prediction,\n",
    "            reference,\n",
    "            custom_prompt=custom_prompt,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
