{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from saga_llm_evaluation_ml.helpers.llm_metrics import GEval, GPTScore, SelfCheckGPT\n",
    "from saga_llm_evaluation_ml.score import LLMScorer\n",
    "from saga_llm_evaluation_ml.helpers.utils import get_llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q3_K_L.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q3_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q3_K - Large\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.35 GiB (4.27 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 3429.86 MB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  500.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 94.33 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "LLAMA_MODEL = get_llama_model(filename=\"llama-2-7b-chat.Q3_K_L.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGEval(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.geval = GEval(model=LLAMA_MODEL)\n",
    "\n",
    "    def test_init(self):\n",
    "        with self.assertRaises(AssertionError):\n",
    "            GEval(LLAMA_MODEL, \"1\", 1)\n",
    "            GEval(LLAMA_MODEL, 1, \"1\")\n",
    "\n",
    "    def test_bad_arguments(self):\n",
    "        source = \"Hi how are you\"\n",
    "        pred = \"Im ok\"\n",
    "        task = \"diag\"\n",
    "        aspect = \"ENG\"\n",
    "\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.geval.compute([source], pred, task, aspect)\n",
    "            self.geval.compute(source, [pred], task, aspect)\n",
    "            self.geval.compute(source, pred, 1, aspect)\n",
    "            self.geval.compute(source, pred, task, 1)\n",
    "            self.geval.compute(source, pred, task, \"notvalid\")\n",
    "            self.geval.compute(source, pred, \"notvalid\", aspect)\n",
    "            self.geval.compute(source, pred, task, aspect=None)\n",
    "            self.geval.compute(source, pred, task=None, aspect=aspect)\n",
    "\n",
    "    def test_compute(self):\n",
    "        source = \"Hi how are you?\"\n",
    "        preds = [\"Shut up creep!!!\", \"I am very good, thank you! And you?\"]\n",
    "        task = \"diag\"\n",
    "        aspect = \"POL\"\n",
    "\n",
    "        scores = {key: 0 for key in preds}\n",
    "        for pred in preds:\n",
    "            score = self.geval.compute(source, pred, task, aspect)\n",
    "            self.assertTrue(isinstance(score, float))\n",
    "            self.assertGreaterEqual(score, 0.0)\n",
    "            scores[pred] = score\n",
    "\n",
    "        self.assertGreaterEqual(\n",
    "            scores[\"I am very good, thank you! And you?\"], scores[\"Shut up creep!!!\"]\n",
    "        )\n",
    "\n",
    "\n",
    "class TestSelfCheckGPT(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.selfcheckgpt = SelfCheckGPT(model=LLAMA_MODEL, eval_model=LLAMA_MODEL)\n",
    "\n",
    "    def test_init(self):\n",
    "        with self.assertRaises(AssertionError):\n",
    "            SelfCheckGPT(\n",
    "                model=LLAMA_MODEL,\n",
    "                eval_model=LLAMA_MODEL,\n",
    "                eval_model_name_or_path=1,\n",
    "                eval_model_basename=1,\n",
    "            )\n",
    "            SelfCheckGPT(\n",
    "                model=LLAMA_MODEL,\n",
    "                eval_model=LLAMA_MODEL,\n",
    "                eval_model_name_or_path=1,\n",
    "                eval_model_basename=\"1\",\n",
    "            )\n",
    "            SelfCheckGPT(\n",
    "                model=LLAMA_MODEL,\n",
    "                eval_model=LLAMA_MODEL,\n",
    "                eval_model_name_or_path=\"1\",\n",
    "                eval_model_basename=1,\n",
    "            )\n",
    "\n",
    "    def test_bad_arguments(self):\n",
    "        question = \"What is the capital of France?\"\n",
    "        pred = \"Paris\"\n",
    "        n_samples = 1\n",
    "\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.selfcheckgpt.compute([question], pred, n_samples)\n",
    "            self.selfcheckgpt.compute(question, [pred], n_samples)\n",
    "            self.selfcheckgpt.compute(question, pred, \"1\")\n",
    "            self.selfcheckgpt.compute(question, pred, 1.0)\n",
    "            self.selfcheckgpt.compute(question, pred, -1)\n",
    "            self.selfcheckgpt.compute(question=question, pred=None, n_samples=5)\n",
    "            self.selfcheckgpt.compute(question=None, pred=pred, n_samples=5)\n",
    "\n",
    "    def test_compute(self):\n",
    "        question = \"What is the capital of France?\"\n",
    "        preds = [\"Paris\", \"sandwich\"]\n",
    "        n_samples = 10\n",
    "\n",
    "        scores = {key: 0 for key in preds}\n",
    "        for pred in preds:\n",
    "            score = self.selfcheckgpt.compute(question, pred, n_samples)\n",
    "            self.assertTrue(isinstance(score, float))\n",
    "            self.assertGreaterEqual(score, 0.0)\n",
    "            self.assertLessEqual(score, 1.0)\n",
    "            scores[pred] = score\n",
    "\n",
    "        self.assertGreaterEqual(scores[\"Paris\"], scores[\"sandwich\"])\n",
    "\n",
    "\n",
    "class TestGPTScore(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gptscore = GPTScore(model=LLAMA_MODEL)\n",
    "\n",
    "    def test_init(self):\n",
    "        with self.assertRaises(AssertionError):\n",
    "            GPTScore(model=LLAMA_MODEL, model_basename=1, model_name_or_path=1)\n",
    "            GPTScore(model=LLAMA_MODEL, model_basename=\"1\", model_name_or_path=1)\n",
    "            GPTScore(model=LLAMA_MODEL, model_basename=1, model_name_or_path=\"1\")\n",
    "\n",
    "    def test_bad_arguments(self):\n",
    "\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.gptscore.compute(\n",
    "                [\"The cat sat on the mat.\"], [\"The dog sat on the log.\"]\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\", [\"The dog sat on the log.\"]\n",
    "            )\n",
    "            self.gptscore.compute(\"The cat sat on the mat.\", \"The dog sat on the log.\")\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\", \"The dog sat on the log.\", prompt=2\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\",\n",
    "                \"The dog sat on the log.\",\n",
    "                prompt=\"2\",\n",
    "                aspect=\"COV\",\n",
    "                task=\"diag\",\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\",\n",
    "                \"The dog sat on the log.\",\n",
    "                aspect=2,\n",
    "                task=\"diag\",\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\",\n",
    "                \"The dog sat on the log.\",\n",
    "                aspect=\"COV\",\n",
    "                task=2,\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\",\n",
    "                \"The dog sat on the log.\",\n",
    "                aspect=\"COV\",\n",
    "                task=\"notvalid\",\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\",\n",
    "                \"The dog sat on the log.\",\n",
    "                aspect=\"notvalid\",\n",
    "                task=\"diag\",\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\", \"The dog sat on the log.\", aspect=\"COV\"\n",
    "            )\n",
    "            self.gptscore.compute(\n",
    "                \"The cat sat on the mat.\", \"The dog sat on the log.\", task=\"diag\"\n",
    "            )\n",
    "\n",
    "    def test_compute(self):\n",
    "        source = \"Hi how are you?\"\n",
    "        preds = [\n",
    "            \"I am very fine. Thanks! What about you?\",\n",
    "            \"Shut up creep I don't want to talk to you!!!\",\n",
    "        ]\n",
    "        # prompt = \"Task: evaluate how polite this dialog is.\"\n",
    "        aspect = \"LIK\"\n",
    "        task = \"diag\"\n",
    "\n",
    "        scores = {key: 0 for key in preds}\n",
    "        for target in preds:\n",
    "            score = self.gptscore.compute(source, target, aspect=aspect, task=task)\n",
    "            scores[target] = score\n",
    "            self.assertTrue(isinstance(score, float))\n",
    "            self.assertGreaterEqual(score, 0.0)\n",
    "\n",
    "        self.assertGreaterEqual(\n",
    "            scores[\"I am very fine. Thanks! What about you?\"],\n",
    "            scores[\"Shut up creep I don't want to talk to you!!!\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_bad_arguments (__main__.TestGEval) ... ok\n",
      "test_compute (__main__.TestGEval) ... \n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    97.04 ms /   197 runs   (    0.49 ms per token,  2030.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5785.84 ms /   198 tokens (   29.22 ms per token,    34.22 tokens per second)\n",
      "llama_print_timings:        eval time = 19715.53 ms /   196 runs   (  100.59 ms per token,     9.94 tokens per second)\n",
      "llama_print_timings:       total time = 26278.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.46 ms per token,  2152.85 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13162.97 ms /   433 tokens (   30.40 ms per token,    32.90 tokens per second)\n",
      "llama_print_timings:        eval time =   101.63 ms /     1 runs   (  101.63 ms per token,     9.84 tokens per second)\n",
      "llama_print_timings:       total time = 13960.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    81.21 ms /   162 runs   (    0.50 ms per token,  1994.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   193.62 ms /     6 tokens (   32.27 ms per token,    30.99 tokens per second)\n",
      "llama_print_timings:        eval time = 16163.58 ms /   161 runs   (  100.39 ms per token,     9.96 tokens per second)\n",
      "llama_print_timings:       total time = 16763.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    19.46 ms /    37 runs   (    0.53 ms per token,  1901.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12085.83 ms /   403 tokens (   29.99 ms per token,    33.34 tokens per second)\n",
      "llama_print_timings:        eval time =  3718.78 ms /    36 runs   (  103.30 ms per token,     9.68 tokens per second)\n",
      "llama_print_timings:       total time = 16520.68 ms\n",
      "ok\n",
      "test_init (__main__.TestGEval) ... ok\n",
      "test_bad_arguments (__main__.TestGPTScore) ... ok\n",
      "test_compute (__main__.TestGPTScore) ... Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    31.02 ms /    61 runs   (    0.51 ms per token,  1966.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1963.30 ms /    68 tokens (   28.87 ms per token,    34.64 tokens per second)\n",
      "llama_print_timings:        eval time =  5930.70 ms /    60 runs   (   98.85 ms per token,    10.12 tokens per second)\n",
      "llama_print_timings:       total time =  8154.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    20.12 ms /    39 runs   (    0.52 ms per token,  1937.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   535.82 ms /    18 tokens (   29.77 ms per token,    33.59 tokens per second)\n",
      "llama_print_timings:        eval time =  3770.07 ms /    38 runs   (   99.21 ms per token,    10.08 tokens per second)\n",
      "llama_print_timings:       total time =  4430.72 ms\n",
      "ok\n",
      "test_init (__main__.TestGPTScore) ... ok\n",
      "test_bad_arguments (__main__.TestSelfCheckGPT) ... ok\n",
      "test_compute (__main__.TestSelfCheckGPT) ... Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    14.59 ms /    31 runs   (    0.47 ms per token,  2124.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1236.28 ms /    42 tokens (   29.44 ms per token,    33.97 tokens per second)\n",
      "llama_print_timings:        eval time =  2909.87 ms /    30 runs   (   97.00 ms per token,    10.31 tokens per second)\n",
      "llama_print_timings:       total time =  4286.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     3.75 ms /     8 runs   (    0.47 ms per token,  2136.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   756.48 ms /     8 runs   (   94.56 ms per token,    10.58 tokens per second)\n",
      "llama_print_timings:       total time =   775.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     3.74 ms /     8 runs   (    0.47 ms per token,  2139.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   767.41 ms /     8 runs   (   95.93 ms per token,    10.42 tokens per second)\n",
      "llama_print_timings:       total time =   786.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     4.29 ms /     8 runs   (    0.54 ms per token,  1863.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   766.70 ms /     8 runs   (   95.84 ms per token,    10.43 tokens per second)\n",
      "llama_print_timings:       total time =   787.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     6.23 ms /    13 runs   (    0.48 ms per token,  2086.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1226.14 ms /    13 runs   (   94.32 ms per token,    10.60 tokens per second)\n",
      "llama_print_timings:       total time =  1257.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     3.89 ms /     8 runs   (    0.49 ms per token,  2054.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   782.83 ms /     8 runs   (   97.85 ms per token,    10.22 tokens per second)\n",
      "llama_print_timings:       total time =   802.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    42.13 ms /    88 runs   (    0.48 ms per token,  2088.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8575.91 ms /    88 runs   (   97.45 ms per token,    10.26 tokens per second)\n",
      "llama_print_timings:       total time =  8800.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    14.69 ms /    31 runs   (    0.47 ms per token,  2110.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2979.54 ms /    31 runs   (   96.11 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:       total time =  3054.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    11.36 ms /    24 runs   (    0.47 ms per token,  2113.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2286.40 ms /    24 runs   (   95.27 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =  2345.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     3.89 ms /     8 runs   (    0.49 ms per token,  2054.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   776.30 ms /     8 runs   (   97.04 ms per token,    10.31 tokens per second)\n",
      "llama_print_timings:       total time =   795.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2145.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3351.24 ms /   115 tokens (   29.14 ms per token,    34.32 tokens per second)\n",
      "llama_print_timings:        eval time =   117.20 ms /     1 runs   (  117.20 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:       total time =  3652.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.04 ms /     2 runs   (    0.52 ms per token,  1930.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   760.96 ms /    26 tokens (   29.27 ms per token,    34.17 tokens per second)\n",
      "llama_print_timings:        eval time =    96.04 ms /     1 runs   (   96.04 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =   900.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2139.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   192.83 ms /     2 runs   (   96.42 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =   198.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2143.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   191.97 ms /     2 runs   (   95.99 ms per token,    10.42 tokens per second)\n",
      "llama_print_timings:       total time =   196.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.03 ms /     2 runs   (    0.52 ms per token,  1937.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   744.25 ms /    25 tokens (   29.77 ms per token,    33.59 tokens per second)\n",
      "llama_print_timings:        eval time =   100.81 ms /     1 runs   (  100.81 ms per token,     9.92 tokens per second)\n",
      "llama_print_timings:       total time =   885.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2143.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   595.50 ms /    20 tokens (   29.77 ms per token,    33.59 tokens per second)\n",
      "llama_print_timings:        eval time =    94.95 ms /     1 runs   (   94.95 ms per token,    10.53 tokens per second)\n",
      "llama_print_timings:       total time =   722.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.04 ms /     2 runs   (    0.52 ms per token,  1913.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2935.34 ms /    99 tokens (   29.65 ms per token,    33.73 tokens per second)\n",
      "llama_print_timings:        eval time =    98.04 ms /     1 runs   (   98.04 ms per token,    10.20 tokens per second)\n",
      "llama_print_timings:       total time =  3195.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.06 ms /     2 runs   (    0.53 ms per token,  1879.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1461.68 ms /    49 tokens (   29.83 ms per token,    33.52 tokens per second)\n",
      "llama_print_timings:        eval time =    96.50 ms /     1 runs   (   96.50 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:       total time =  1641.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.43 ms /     3 runs   (    0.48 ms per token,  2094.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1247.98 ms /    42 tokens (   29.71 ms per token,    33.65 tokens per second)\n",
      "llama_print_timings:        eval time =   193.36 ms /     2 runs   (   96.68 ms per token,    10.34 tokens per second)\n",
      "llama_print_timings:       total time =  1515.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.05 ms /     2 runs   (    0.52 ms per token,  1908.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   602.77 ms /    20 tokens (   30.14 ms per token,    33.18 tokens per second)\n",
      "llama_print_timings:        eval time =   119.35 ms /     1 runs   (  119.35 ms per token,     8.38 tokens per second)\n",
      "llama_print_timings:       total time =   756.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     9.89 ms /    19 runs   (    0.52 ms per token,  1921.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   980.90 ms /    33 tokens (   29.72 ms per token,    33.64 tokens per second)\n",
      "llama_print_timings:        eval time =  1729.63 ms /    18 runs   (   96.09 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =  2809.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    35.24 ms /    73 runs   (    0.48 ms per token,  2071.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7048.57 ms /    73 runs   (   96.56 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:       total time =  7229.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    13.36 ms /    27 runs   (    0.49 ms per token,  2021.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2608.66 ms /    27 runs   (   96.62 ms per token,    10.35 tokens per second)\n",
      "llama_print_timings:       total time =  2675.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     5.70 ms /    12 runs   (    0.47 ms per token,  2106.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1139.38 ms /    12 runs   (   94.95 ms per token,    10.53 tokens per second)\n",
      "llama_print_timings:       total time =  1167.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     6.94 ms /    13 runs   (    0.53 ms per token,  1872.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1278.57 ms /    13 runs   (   98.35 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:       total time =  1312.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     9.31 ms /    17 runs   (    0.55 ms per token,  1826.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1664.46 ms /    17 runs   (   97.91 ms per token,    10.21 tokens per second)\n",
      "llama_print_timings:       total time =  1710.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     3.82 ms /     8 runs   (    0.48 ms per token,  2095.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   764.09 ms /     8 runs   (   95.51 ms per token,    10.47 tokens per second)\n",
      "llama_print_timings:       total time =   782.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =    34.49 ms /    71 runs   (    0.49 ms per token,  2058.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6906.87 ms /    71 runs   (   97.28 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =  7085.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     5.37 ms /     9 runs   (    0.60 ms per token,  1676.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   901.55 ms /     9 runs   (  100.17 ms per token,     9.98 tokens per second)\n",
      "llama_print_timings:       total time =   928.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     4.52 ms /     8 runs   (    0.57 ms per token,  1768.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   797.83 ms /     8 runs   (   99.73 ms per token,    10.03 tokens per second)\n",
      "llama_print_timings:       total time =   819.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.03 ms /     2 runs   (    0.51 ms per token,  1947.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3007.69 ms /   104 tokens (   28.92 ms per token,    34.58 tokens per second)\n",
      "llama_print_timings:        eval time =    95.68 ms /     1 runs   (   95.68 ms per token,    10.45 tokens per second)\n",
      "llama_print_timings:       total time =  3270.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.68 ms /     2 runs   (    0.84 ms per token,  1193.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2511.46 ms /    85 tokens (   29.55 ms per token,    33.84 tokens per second)\n",
      "llama_print_timings:        eval time =   121.82 ms /     1 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  2774.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2148.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1331.40 ms /    46 tokens (   28.94 ms per token,    34.55 tokens per second)\n",
      "llama_print_timings:        eval time =   102.92 ms /     1 runs   (  102.92 ms per token,     9.72 tokens per second)\n",
      "llama_print_timings:       total time =  1511.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2145.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   596.17 ms /    20 tokens (   29.81 ms per token,    33.55 tokens per second)\n",
      "llama_print_timings:        eval time =    91.75 ms /     1 runs   (   91.75 ms per token,    10.90 tokens per second)\n",
      "llama_print_timings:       total time =   719.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.47 ms /     1 runs   (    0.47 ms per token,  2127.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   959.76 ms /    32 tokens (   29.99 ms per token,    33.34 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  1011.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2127.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   767.80 ms /    26 tokens (   29.53 ms per token,    33.86 tokens per second)\n",
      "llama_print_timings:        eval time =    96.89 ms /     1 runs   (   96.89 ms per token,    10.32 tokens per second)\n",
      "llama_print_timings:       total time =   906.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.74 ms /     2 runs   (    0.87 ms per token,  1146.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   625.63 ms /    21 tokens (   29.79 ms per token,    33.57 tokens per second)\n",
      "llama_print_timings:        eval time =    96.64 ms /     1 runs   (   96.64 ms per token,    10.35 tokens per second)\n",
      "llama_print_timings:       total time =   761.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     1.95 ms /     4 runs   (    0.49 ms per token,  2049.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2426.48 ms /    83 tokens (   29.23 ms per token,    34.21 tokens per second)\n",
      "llama_print_timings:        eval time =   298.75 ms /     3 runs   (   99.58 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =  2869.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.47 ms /     1 runs   (    0.47 ms per token,  2141.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   625.47 ms /    21 tokens (   29.78 ms per token,    33.58 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   656.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "FAIL\n",
      "llama_print_timings:        load time =  5785.98 ms\n",
      "llama_print_timings:      sample time =     0.47 ms /     1 runs   (    0.47 ms per token,  2141.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   592.88 ms /    20 tokens (   29.64 ms per token,    33.73 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   623.19 ms\n",
      "test_init (__main__.TestSelfCheckGPT) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_compute (__main__.TestSelfCheckGPT)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_135/2500125693.py\", line 98, in test_compute\n",
      "    self.assertGreaterEqual(scores[\"Paris\"], scores[\"sandwich\"])\n",
      "AssertionError: 0.4 not greater than or equal to 0.6\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 219.597s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f823d152710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[\"\"], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLLMScorer(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scorer = LLMScorer(model=LLAMA_MODEL)\n",
    "\n",
    "    def test_init(self):\n",
    "        false = False\n",
    "        with self.assertRaises(AssertionError):\n",
    "\n",
    "            LLMScorer(model=LLAMA_MODEL, lan=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, bleurt_model=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, mauve_model=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, selfcheckgpt_eval_model_name_or_path=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, selfcheckgpt_eval_model_basename=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, geval_model_name_or_path=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, geval_model_basename=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, gptscore_model_name_or_path=false)\n",
    "            LLMScorer(model=LLAMA_MODEL, gptscore_model_basename=false)\n",
    "\n",
    "    def test_score_bad_arguments(self):\n",
    "        llm_input = \"I am a dog.\"\n",
    "        prompt = f\"System: You are a cat. You don't like dogs. User: {llm_input}\"\n",
    "        context = \"System: You are a cat. You don't like dogs.\"\n",
    "        prediction = \"I am a cat, I don't like dogs.\"\n",
    "        reference = \"I am a cat, I don't like dogs, miau.\"\n",
    "\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.scorer.score(False, prompt, context, prediction, reference)\n",
    "            self.scorer.score(llm_input, False, context, prediction, reference)\n",
    "            self.scorer.score(llm_input, prompt, False, prediction, reference)\n",
    "            self.scorer.score(llm_input, prompt, context, False, reference)\n",
    "            self.scorer.score(llm_input, prompt, context, prediction, False)\n",
    "            self.scorer.score(\n",
    "                llm_input, prompt, context, prediction, reference, n_samples=False\n",
    "            )\n",
    "            self.scorer.score(\n",
    "                llm_input, prompt, context, prediction, reference, task=False\n",
    "            )\n",
    "            self.scorer.score(\n",
    "                llm_input, prompt, context, prediction, reference, aspects=False\n",
    "            )\n",
    "            self.scorer.score(\n",
    "                llm_input, prompt, context, prediction, reference, custom_prompt=False\n",
    "            )\n",
    "            self.scorer.score(\n",
    "                llm_input, prompt, context, prediction, reference, custom_prompt=False\n",
    "            )\n",
    "\n",
    "    def test_score(self):\n",
    "        model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
    "        model_basename = \"llama-2-7b-chat.Q2_K.gguf\"  # the model is in bin format\n",
    "\n",
    "        scorer = LLMScorer(\n",
    "            model=LLAMA_MODEL,\n",
    "            selfcheckgpt_eval_model_name_or_path=model_name_or_path,\n",
    "            selfcheckgpt_eval_model_basename=model_basename,\n",
    "            geval_model_name_or_path=model_name_or_path,\n",
    "            geval_model_basename=model_basename,\n",
    "            gptscore_model_name_or_path=model_name_or_path,\n",
    "            gptscore_model_basename=model_basename,\n",
    "        )\n",
    "\n",
    "        llm_input = \"I am a dog.\"\n",
    "        prompt = f\"System: You are a cat. You don't like dogs. User: {llm_input}\"\n",
    "        context = \"Examples: Eww, I hate dogs.\"\n",
    "        prediction = \"I am a cat, I don't like dogs.\"\n",
    "        reference = \"I am a cat, I don't like dogs, miau.\"\n",
    "        task = \"diag\"\n",
    "        aspect = [\"CON\"]\n",
    "        custom_prompt = {\n",
    "            \"name\": \"Fluency\",\n",
    "            \"task\": \"Dialog\",\n",
    "            \"aspect\": \"Evaluate the fluency of the following dialog.\",\n",
    "        }\n",
    "\n",
    "        scores = scorer.score(llm_input, prompt, context, prediction, reference)\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"metrics\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # All default\n",
    "        print(\"All default\")\n",
    "        scores = scorer.score(llm_input, prompt, prediction, n_samples=2)\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"metrics\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # All default, but with context\n",
    "        print(\"All default, but with context\")\n",
    "        scores = scorer.score(\n",
    "            llm_input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "            context=context,\n",
    "            n_samples=2,\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"metrics\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # All default, but with reference\n",
    "        print(\"All default, but with reference\")\n",
    "        scores = scorer.score(\n",
    "            llm_input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "            reference=reference,\n",
    "            n_samples=2,\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"metrics\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # Precise task and aspect\n",
    "        print(\"Precise task and aspect\")\n",
    "        scores = scorer.score(\n",
    "            llm_input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "            task=task,\n",
    "            aspects=aspect,\n",
    "            n_samples=2,\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"metrics\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n",
    "\n",
    "        # Precise custom prompt\n",
    "        print(\"Precise custom prompt\")\n",
    "        scores = scorer.score(\n",
    "            llm_input,\n",
    "            prompt,\n",
    "            prediction,\n",
    "            custom_prompt=custom_prompt,\n",
    "            n_samples=2,\n",
    "        )\n",
    "        self.assertTrue(isinstance(scores, dict))\n",
    "        self.assertTrue(\"metrics\" in scores)\n",
    "        self.assertTrue(\"metadata\" in scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|| 7.95k/7.95k [00:00<00:00, 10.4MB/s]\n",
      "Downloading builder script: 100%|| 7.24k/7.24k [00:00<00:00, 10.9MB/s]\n",
      "Downloading builder script: 100%|| 5.20k/5.20k [00:00<00:00, 6.52MB/s]\n",
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n",
      "Downloading data: 100%|| 405M/405M [00:30<00:00, 13.4MB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n",
      "()ad-v2/resolve/main/tokenizer_config.json: 100%|| 58.0/58.0 [00:00<00:00, 169kB/s]\n",
      "()rge-v2-squad-v2/resolve/main/config.json: 100%|| 717/717 [00:00<00:00, 2.48MB/s]\n",
      "()ge-v2-squad-v2/resolve/main/spiece.model: 100%|| 760k/760k [00:00<00:00, 937kB/s]\n",
      "()-squad-v2/resolve/main/added_tokens.json: 100%|| 2.00/2.00 [00:00<00:00, 9.97kB/s]\n",
      "()-v2/resolve/main/special_tokens_map.json: 100%|| 156/156 [00:00<00:00, 382kB/s]\n",
      "pytorch_model.bin: 100%|| 235M/235M [00:17<00:00, 13.8MB/s] \n",
      "Some weights of the model checkpoint at ktrapeznikov/albert-xlarge-v2-squad-v2 were not used when initializing AlbertForQuestionAnswering: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "()on-ap/resolve/main/tokenizer_config.json: 100%|| 25.0/25.0 [00:00<00:00, 141kB/s]\n",
      "()n-generation-ap/resolve/main/config.json: 100%|| 1.23k/1.23k [00:00<00:00, 6.35MB/s]\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "()-generation-ap/resolve/main/spiece.model: 100%|| 792k/792k [00:00<00:00, 5.69MB/s]\n",
      "()-ap/resolve/main/special_tokens_map.json: 100%|| 1.79k/1.79k [00:00<00:00, 8.96MB/s]\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "/opt/poetry-cache/virtualenvs/saga-llm-evaluation-ml-8EXZSVYp-py3.10/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1479: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "model.safetensors: 100%|| 1.19G/1.19G [01:27<00:00, 13.6MB/s]\n",
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n",
      "Some weights of the model checkpoint at ktrapeznikov/albert-xlarge-v2-squad-v2 were not used when initializing AlbertForQuestionAnswering: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n",
      "Some weights of the model checkpoint at ktrapeznikov/albert-xlarge-v2-squad-v2 were not used when initializing AlbertForQuestionAnswering: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "test_bad_arguments (__main__.TestGEval) ... ok\n",
      "test_compute (__main__.TestGEval) ... \n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    54.72 ms /   111 runs   (    0.49 ms per token,  2028.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5726.71 ms /   198 tokens (   28.92 ms per token,    34.57 tokens per second)\n",
      "llama_print_timings:        eval time = 11050.08 ms /   110 runs   (  100.46 ms per token,     9.95 tokens per second)\n",
      "llama_print_timings:       total time = 17380.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    16.93 ms /    32 runs   (    0.53 ms per token,  1890.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10564.43 ms /   347 tokens (   30.45 ms per token,    32.85 tokens per second)\n",
      "llama_print_timings:        eval time =  3213.51 ms /    31 runs   (  103.66 ms per token,     9.65 tokens per second)\n",
      "llama_print_timings:       total time = 14446.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =   100.73 ms /   202 runs   (    0.50 ms per token,  2005.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   191.36 ms /     6 tokens (   31.89 ms per token,    31.35 tokens per second)\n",
      "llama_print_timings:        eval time = 20393.83 ms /   201 runs   (  101.46 ms per token,     9.86 tokens per second)\n",
      "llama_print_timings:       total time = 21084.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2143.62 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13545.02 ms /   442 tokens (   30.64 ms per token,    32.63 tokens per second)\n",
      "llama_print_timings:        eval time =   105.30 ms /     1 runs   (  105.30 ms per token,     9.50 tokens per second)\n",
      "llama_print_timings:       total time = 14402.67 ms\n",
      "ok\n",
      "test_init (__main__.TestGEval) ... ok\n",
      "test_bad_arguments (__main__.TestGPTScore) ... ok\n",
      "test_compute (__main__.TestGPTScore) ... Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     2.36 ms /     5 runs   (    0.47 ms per token,  2121.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1966.06 ms /    68 tokens (   28.91 ms per token,    34.59 tokens per second)\n",
      "llama_print_timings:        eval time =   379.40 ms /     4 runs   (   94.85 ms per token,    10.54 tokens per second)\n",
      "llama_print_timings:       total time =  2467.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     2.50 ms /     5 runs   (    0.50 ms per token,  1998.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   519.60 ms /    18 tokens (   28.87 ms per token,    34.64 tokens per second)\n",
      "llama_print_timings:        eval time =   383.84 ms /     4 runs   (   95.96 ms per token,    10.42 tokens per second)\n",
      "llama_print_timings:       total time =   939.68 ms\n",
      "ok\n",
      "test_init (__main__.TestGPTScore) ... ok\n",
      "test_init (__main__.TestLLMScorer) ... ok\n",
      "test_score (__main__.TestLLMScorer) ... WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n",
      "Some weights of the model checkpoint at ktrapeznikov/albert-xlarge-v2-squad-v2 were not used when initializing AlbertForQuestionAnswering: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "/opt/poetry-cache/virtualenvs/saga-llm-evaluation-ml-8EXZSVYp-py3.10/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1479: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "()cased/resolve/main/tokenizer_config.json: 100%|| 28.0/28.0 [00:00<00:00, 68.9kB/s]\n",
      "()rt-base-uncased/resolve/main/config.json: 100%|| 483/483 [00:00<00:00, 1.56MB/s]\n",
      "()bert-base-uncased/resolve/main/vocab.txt: 100%|| 232k/232k [00:00<00:00, 8.75MB/s]\n",
      "model.safetensors: 100%|| 268M/268M [00:15<00:00, 17.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "()ingface.co/gpt2/resolve/main/config.json: 100%|| 665/665 [00:00<00:00, 3.59MB/s]\n",
      "()gingface.co/gpt2/resolve/main/vocab.json: 100%|| 1.04M/1.04M [00:00<00:00, 3.55MB/s]\n",
      "()gingface.co/gpt2/resolve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 2.53MB/s]\n",
      "()face.co/gpt2/resolve/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 4.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Loading tokenizer\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|| 548M/548M [00:37<00:00, 14.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|| 1/1 [00:00<00:00, 44.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing q: 100%|| 1/1 [00:00<00:00, 39.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 25\n",
      "performing clustering in lower dimension = 0\n",
      "kmeans time: 0.0 s\n",
      "total discretization time: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING clustering 2 points to 2 centroids: please provide at least 78 training points\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    22.38 ms /    45 runs   (    0.50 ms per token,  2010.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1141.50 ms /    40 tokens (   28.54 ms per token,    35.04 tokens per second)\n",
      "llama_print_timings:        eval time =  4289.00 ms /    44 runs   (   97.48 ms per token,    10.26 tokens per second)\n",
      "llama_print_timings:       total time =  5595.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    21.53 ms /    44 runs   (    0.49 ms per token,  2044.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4295.02 ms /    44 runs   (   97.61 ms per token,    10.24 tokens per second)\n",
      "llama_print_timings:       total time =  4394.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    25.46 ms /    51 runs   (    0.50 ms per token,  2003.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4989.89 ms /    51 runs   (   97.84 ms per token,    10.22 tokens per second)\n",
      "llama_print_timings:       total time =  5106.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    25.73 ms /    49 runs   (    0.53 ms per token,  1904.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4780.53 ms /    49 runs   (   97.56 ms per token,    10.25 tokens per second)\n",
      "llama_print_timings:       total time =  4897.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    21.93 ms /    43 runs   (    0.51 ms per token,  1961.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4193.37 ms /    43 runs   (   97.52 ms per token,    10.25 tokens per second)\n",
      "llama_print_timings:       total time =  4292.06 ms\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2132.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3488.48 ms /   146 tokens (   23.89 ms per token,    41.85 tokens per second)\n",
      "llama_print_timings:        eval time =   101.43 ms /     1 runs   (  101.43 ms per token,     9.86 tokens per second)\n",
      "llama_print_timings:       total time =  3840.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2139.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1745.55 ms /    71 tokens (   24.59 ms per token,    40.67 tokens per second)\n",
      "llama_print_timings:        eval time =    93.98 ms /     1 runs   (   93.98 ms per token,    10.64 tokens per second)\n",
      "llama_print_timings:       total time =  1967.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2120.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1871.10 ms /    78 tokens (   23.99 ms per token,    41.69 tokens per second)\n",
      "llama_print_timings:        eval time =    89.05 ms /     1 runs   (   89.05 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =  2092.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2139.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1767.96 ms /    74 tokens (   23.89 ms per token,    41.86 tokens per second)\n",
      "llama_print_timings:        eval time =    99.03 ms /     1 runs   (   99.03 ms per token,    10.10 tokens per second)\n",
      "llama_print_timings:       total time =  1989.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2132.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1652.59 ms /    68 tokens (   24.30 ms per token,    41.15 tokens per second)\n",
      "llama_print_timings:        eval time =    92.15 ms /     1 runs   (   92.15 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =  1857.06 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    15.26 ms /    32 runs   (    0.48 ms per token,  2096.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3059.18 ms /    32 runs   (   95.60 ms per token,    10.46 tokens per second)\n",
      "llama_print_timings:       total time =  3128.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    30.78 ms /    62 runs   (    0.50 ms per token,  2014.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6059.56 ms /    62 runs   (   97.73 ms per token,    10.23 tokens per second)\n",
      "llama_print_timings:       total time =  6199.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     7.33 ms /    14 runs   (    0.52 ms per token,  1910.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1483.35 ms /    61 tokens (   24.32 ms per token,    41.12 tokens per second)\n",
      "llama_print_timings:        eval time =  1186.45 ms /    13 runs   (   91.27 ms per token,    10.96 tokens per second)\n",
      "llama_print_timings:       total time =  2802.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2136.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1923.15 ms /    82 tokens (   23.45 ms per token,    42.64 tokens per second)\n",
      "llama_print_timings:        eval time =    88.92 ms /     1 runs   (   88.92 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =  2151.51 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All default, but with context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    25.07 ms /    52 runs   (    0.48 ms per token,  2074.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4974.52 ms /    52 runs   (   95.66 ms per token,    10.45 tokens per second)\n",
      "llama_print_timings:       total time =  5091.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    27.49 ms /    54 runs   (    0.51 ms per token,  1964.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5328.76 ms /    54 runs   (   98.68 ms per token,    10.13 tokens per second)\n",
      "llama_print_timings:       total time =  5453.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2125.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2012.63 ms /    81 tokens (   24.85 ms per token,    40.25 tokens per second)\n",
      "llama_print_timings:        eval time =   128.09 ms /     1 runs   (  128.09 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:       total time =  2274.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2145.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2049.85 ms /    83 tokens (   24.70 ms per token,    40.49 tokens per second)\n",
      "llama_print_timings:        eval time =    89.80 ms /     1 runs   (   89.80 ms per token,    11.14 tokens per second)\n",
      "llama_print_timings:       total time =  2280.34 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All default, but with reference\n",
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|| 1/1 [00:00<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing q: 100%|| 1/1 [00:00<00:00, 35.93it/s]\n",
      "WARNING clustering 2 points to 2 centroids: please provide at least 78 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 25\n",
      "performing clustering in lower dimension = 0\n",
      "kmeans time: 0.0 s\n",
      "total discretization time: 0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    17.61 ms /    36 runs   (    0.49 ms per token,  2044.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3562.06 ms /    36 runs   (   98.95 ms per token,    10.11 tokens per second)\n",
      "llama_print_timings:       total time =  3645.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    25.82 ms /    53 runs   (    0.49 ms per token,  2052.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5136.73 ms /    53 runs   (   96.92 ms per token,    10.32 tokens per second)\n",
      "llama_print_timings:       total time =  5258.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2136.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1604.59 ms /    65 tokens (   24.69 ms per token,    40.51 tokens per second)\n",
      "llama_print_timings:        eval time =   109.78 ms /     1 runs   (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:       total time =  1821.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     8.93 ms /    19 runs   (    0.47 ms per token,  2128.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1942.38 ms /    81 tokens (   23.98 ms per token,    41.70 tokens per second)\n",
      "llama_print_timings:        eval time =  1648.75 ms /    18 runs   (   91.60 ms per token,    10.92 tokens per second)\n",
      "llama_print_timings:       total time =  3779.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise task and aspect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =    29.05 ms /    56 runs   (    0.52 ms per token,  1927.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3798.19 ms /   160 tokens (   23.74 ms per token,    42.13 tokens per second)\n",
      "llama_print_timings:        eval time =  5028.43 ms /    55 runs   (   91.43 ms per token,    10.94 tokens per second)\n",
      "llama_print_timings:       total time =  9227.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     3.30 ms /     7 runs   (    0.47 ms per token,  2119.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6852.10 ms /   277 tokens (   24.74 ms per token,    40.43 tokens per second)\n",
      "llama_print_timings:        eval time =   598.94 ms /     6 runs   (   99.82 ms per token,    10.02 tokens per second)\n",
      "llama_print_timings:       total time =  7935.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     2.44 ms /     5 runs   (    0.49 ms per token,  2047.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2129.17 ms /    90 tokens (   23.66 ms per token,    42.27 tokens per second)\n",
      "llama_print_timings:        eval time =   365.53 ms /     4 runs   (   91.38 ms per token,    10.94 tokens per second)\n",
      "llama_print_timings:       total time =  2655.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    27.23 ms /    55 runs   (    0.50 ms per token,  2019.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5283.18 ms /    55 runs   (   96.06 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =  5415.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    16.23 ms /    32 runs   (    0.51 ms per token,  1971.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3052.30 ms /    32 runs   (   95.38 ms per token,    10.48 tokens per second)\n",
      "llama_print_timings:       total time =  3129.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     1.38 ms /     3 runs   (    0.46 ms per token,  2167.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3764.22 ms /   157 tokens (   23.98 ms per token,    41.71 tokens per second)\n",
      "llama_print_timings:        eval time =   176.13 ms /     2 runs   (   88.06 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =  4209.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2141.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1437.28 ms /    59 tokens (   24.36 ms per token,    41.05 tokens per second)\n",
      "llama_print_timings:        eval time =    93.94 ms /     1 runs   (   93.94 ms per token,    10.64 tokens per second)\n",
      "llama_print_timings:       total time =  1629.03 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise custom prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    29.37 ms /    59 runs   (    0.50 ms per token,  2009.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5675.74 ms /    59 runs   (   96.20 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:       total time =  5817.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    20.14 ms /    39 runs   (    0.52 ms per token,  1935.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3734.80 ms /    39 runs   (   95.76 ms per token,    10.44 tokens per second)\n",
      "llama_print_timings:       total time =  3832.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.95 ms /     2 runs   (    0.48 ms per token,  2105.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1549.07 ms /    65 tokens (   23.83 ms per token,    41.96 tokens per second)\n",
      "llama_print_timings:        eval time =    89.41 ms /     1 runs   (   89.41 ms per token,    11.18 tokens per second)\n",
      "llama_print_timings:       total time =  1747.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =     0.95 ms /     2 runs   (    0.47 ms per token,  2107.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1557.64 ms /    62 tokens (   25.12 ms per token,    39.80 tokens per second)\n",
      "llama_print_timings:        eval time =    89.50 ms /     1 runs   (   89.50 ms per token,    11.17 tokens per second)\n",
      "llama_print_timings:       total time =  1751.27 ms\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =    79.90 ms /   161 runs   (    0.50 ms per token,  2015.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   465.81 ms /    19 tokens (   24.52 ms per token,    40.79 tokens per second)\n",
      "llama_print_timings:        eval time = 14399.70 ms /   160 runs   (   90.00 ms per token,    11.11 tokens per second)\n",
      "llama_print_timings:       total time = 15300.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =   119.88 ms /   250 runs   (    0.48 ms per token,  2085.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5924.24 ms /   243 tokens (   24.38 ms per token,    41.02 tokens per second)\n",
      "llama_print_timings:        eval time = 23364.70 ms /   249 runs   (   93.83 ms per token,    10.66 tokens per second)\n",
      "llama_print_timings:       total time = 30356.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3488.56 ms\n",
      "llama_print_timings:      sample time =    17.82 ms /    36 runs   (    0.49 ms per token,  2020.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1650.22 ms /    70 tokens (   23.57 ms per token,    42.42 tokens per second)\n",
      "llama_print_timings:        eval time =  3152.20 ms /    35 runs   (   90.06 ms per token,    11.10 tokens per second)\n",
      "llama_print_timings:       total time =  4998.88 ms\n",
      "ok\n",
      "test_score_bad_arguments (__main__.TestLLMScorer) ... ok\n",
      "test_bad_arguments (__main__.TestSelfCheckGPT) ... ok\n",
      "test_compute (__main__.TestSelfCheckGPT) ... Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.74 ms /     8 runs   (    0.47 ms per token,  2140.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   425.02 ms /    14 tokens (   30.36 ms per token,    32.94 tokens per second)\n",
      "llama_print_timings:        eval time =   683.03 ms /     7 runs   (   97.58 ms per token,    10.25 tokens per second)\n",
      "llama_print_timings:       total time =  1147.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    27.19 ms /    56 runs   (    0.49 ms per token,  2059.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5579.44 ms /    56 runs   (   99.63 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =  5730.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.90 ms /     8 runs   (    0.49 ms per token,  2052.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   800.66 ms /     8 runs   (  100.08 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:       total time =   821.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.82 ms /     8 runs   (    0.48 ms per token,  2095.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   782.90 ms /     8 runs   (   97.86 ms per token,    10.22 tokens per second)\n",
      "llama_print_timings:       total time =   803.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     8.03 ms /    17 runs   (    0.47 ms per token,  2116.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1710.26 ms /    17 runs   (  100.60 ms per token,     9.94 tokens per second)\n",
      "llama_print_timings:       total time =  1754.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.79 ms /     8 runs   (    0.47 ms per token,  2110.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   796.61 ms /     8 runs   (   99.58 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =   817.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     6.53 ms /    12 runs   (    0.54 ms per token,  1838.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1182.27 ms /    12 runs   (   98.52 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =  1217.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    10.03 ms /    20 runs   (    0.50 ms per token,  1994.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2009.54 ms /    20 runs   (  100.48 ms per token,     9.95 tokens per second)\n",
      "llama_print_timings:       total time =  2065.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.96 ms /     8 runs   (    0.49 ms per token,  2020.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   799.84 ms /     8 runs   (   99.98 ms per token,    10.00 tokens per second)\n",
      "llama_print_timings:       total time =   820.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     6.29 ms /    13 runs   (    0.48 ms per token,  2066.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1281.29 ms /    13 runs   (   98.56 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =  1316.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.00 ms /     2 runs   (    0.50 ms per token,  2008.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2675.70 ms /    92 tokens (   29.08 ms per token,    34.38 tokens per second)\n",
      "llama_print_timings:        eval time =    95.22 ms /     1 runs   (   95.22 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =  2925.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.01 ms /     2 runs   (    0.51 ms per token,  1976.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1975.17 ms /    67 tokens (   29.48 ms per token,    33.92 tokens per second)\n",
      "llama_print_timings:        eval time =    99.04 ms /     1 runs   (   99.04 ms per token,    10.10 tokens per second)\n",
      "llama_print_timings:       total time =  2187.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2148.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   562.33 ms /    19 tokens (   29.60 ms per token,    33.79 tokens per second)\n",
      "llama_print_timings:        eval time =    95.06 ms /     1 runs   (   95.06 ms per token,    10.52 tokens per second)\n",
      "llama_print_timings:       total time =   689.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.47 ms /     1 runs   (    0.47 ms per token,  2118.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.72 ms /     1 runs   (   93.72 ms per token,    10.67 tokens per second)\n",
      "llama_print_timings:       total time =    96.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2136.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   861.81 ms /    29 tokens (   29.72 ms per token,    33.65 tokens per second)\n",
      "llama_print_timings:        eval time =    95.77 ms /     1 runs   (   95.77 ms per token,    10.44 tokens per second)\n",
      "llama_print_timings:       total time =  1007.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.03 ms /     2 runs   (    0.51 ms per token,  1949.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   615.79 ms /    20 tokens (   30.79 ms per token,    32.48 tokens per second)\n",
      "llama_print_timings:        eval time =    95.21 ms /     1 runs   (   95.21 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =   745.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.03 ms /     2 runs   (    0.52 ms per token,  1937.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   892.13 ms /    30 tokens (   29.74 ms per token,    33.63 tokens per second)\n",
      "llama_print_timings:        eval time =   103.77 ms /     1 runs   (  103.77 ms per token,     9.64 tokens per second)\n",
      "llama_print_timings:       total time =  1048.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.53 ms /     3 runs   (    0.51 ms per token,  1959.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1125.14 ms /    38 tokens (   29.61 ms per token,    33.77 tokens per second)\n",
      "llama_print_timings:        eval time =   205.40 ms /     2 runs   (  102.70 ms per token,     9.74 tokens per second)\n",
      "llama_print_timings:       total time =  1407.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.99 ms /     4 runs   (    0.50 ms per token,  2008.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   573.89 ms /    20 tokens (   28.69 ms per token,    34.85 tokens per second)\n",
      "llama_print_timings:        eval time =   333.18 ms /     3 runs   (  111.06 ms per token,     9.00 tokens per second)\n",
      "llama_print_timings:       total time =   950.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.97 ms /     2 runs   (    0.48 ms per token,  2066.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   739.66 ms /    25 tokens (   29.59 ms per token,    33.80 tokens per second)\n",
      "llama_print_timings:        eval time =    94.98 ms /     1 runs   (   94.98 ms per token,    10.53 tokens per second)\n",
      "llama_print_timings:       total time =   878.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.81 ms /     8 runs   (    0.48 ms per token,  2101.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   973.17 ms /    33 tokens (   29.49 ms per token,    33.91 tokens per second)\n",
      "llama_print_timings:        eval time =   705.53 ms /     7 runs   (  100.79 ms per token,     9.92 tokens per second)\n",
      "llama_print_timings:       total time =  1756.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.90 ms /     8 runs   (    0.49 ms per token,  2049.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   786.96 ms /     8 runs   (   98.37 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:       total time =   812.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    10.78 ms /    22 runs   (    0.49 ms per token,  2041.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2232.89 ms /    22 runs   (  101.50 ms per token,     9.85 tokens per second)\n",
      "llama_print_timings:       total time =  2305.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.93 ms /     8 runs   (    0.49 ms per token,  2033.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   799.19 ms /     8 runs   (   99.90 ms per token,    10.01 tokens per second)\n",
      "llama_print_timings:       total time =   824.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    42.70 ms /    87 runs   (    0.49 ms per token,  2037.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8808.11 ms /    87 runs   (  101.24 ms per token,     9.88 tokens per second)\n",
      "llama_print_timings:       total time =  9075.81 ms\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.91 ms /     8 runs   (    0.49 ms per token,  2043.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   784.17 ms /     8 runs   (   98.02 ms per token,    10.20 tokens per second)\n",
      "llama_print_timings:       total time =   805.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     4.59 ms /     8 runs   (    0.57 ms per token,  1742.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   809.33 ms /     8 runs   (  101.17 ms per token,     9.88 tokens per second)\n",
      "llama_print_timings:       total time =   833.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     3.74 ms /     8 runs   (    0.47 ms per token,  2137.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   785.79 ms /     8 runs   (   98.22 ms per token,    10.18 tokens per second)\n",
      "llama_print_timings:       total time =   807.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =    13.01 ms /    27 runs   (    0.48 ms per token,  2075.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2673.02 ms /    27 runs   (   99.00 ms per token,    10.10 tokens per second)\n",
      "llama_print_timings:       total time =  2747.07 ms\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     8.90 ms /    18 runs   (    0.49 ms per token,  2021.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1792.83 ms /    18 runs   (   99.60 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =  1843.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.02 ms /     2 runs   (    0.51 ms per token,  1966.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2686.40 ms /    93 tokens (   28.89 ms per token,    34.62 tokens per second)\n",
      "llama_print_timings:        eval time =   104.84 ms /     1 runs   (  104.84 ms per token,     9.54 tokens per second)\n",
      "llama_print_timings:       total time =  2947.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.96 ms /     4 runs   (    0.49 ms per token,  2042.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   384.22 ms /     4 runs   (   96.06 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =   394.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.47 ms /     1 runs   (    0.47 ms per token,  2127.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1006.35 ms /    34 tokens (   29.60 ms per token,    33.79 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  1062.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.95 ms /     4 runs   (    0.49 ms per token,  2052.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   591.85 ms /    20 tokens (   29.59 ms per token,    33.79 tokens per second)\n",
      "llama_print_timings:        eval time =   286.00 ms /     3 runs   (   95.33 ms per token,    10.49 tokens per second)\n",
      "llama_print_timings:       total time =   916.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.46 ms per token,  2152.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2905.65 ms /    99 tokens (   29.35 ms per token,    34.07 tokens per second)\n",
      "llama_print_timings:        eval time =   132.08 ms /     1 runs   (  132.08 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:       total time =  3204.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.93 ms /     2 runs   (    0.47 ms per token,  2141.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   580.54 ms /    20 tokens (   29.03 ms per token,    34.45 tokens per second)\n",
      "llama_print_timings:        eval time =    95.98 ms /     1 runs   (   95.98 ms per token,    10.42 tokens per second)\n",
      "llama_print_timings:       total time =   710.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     1.01 ms /     2 runs   (    0.51 ms per token,  1978.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   190.08 ms /     2 runs   (   95.04 ms per token,    10.52 tokens per second)\n",
      "llama_print_timings:       total time =   196.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.47 ms /     1 runs   (    0.47 ms per token,  2150.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    95.53 ms /     1 runs   (   95.53 ms per token,    10.47 tokens per second)\n",
      "llama_print_timings:       total time =    98.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2139.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1369.67 ms /    46 tokens (   29.78 ms per token,    33.58 tokens per second)\n",
      "llama_print_timings:        eval time =   113.28 ms /     1 runs   (  113.28 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:       total time =  1561.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "ok\n",
      "llama_print_timings:        load time =  5726.79 ms\n",
      "llama_print_timings:      sample time =     0.94 ms /     2 runs   (    0.47 ms per token,  2129.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1096.24 ms /    37 tokens (   29.63 ms per token,    33.75 tokens per second)\n",
      "llama_print_timings:        eval time =    97.11 ms /     1 runs   (   97.11 ms per token,    10.30 tokens per second)\n",
      "llama_print_timings:       total time =  1267.46 ms\n",
      "test_init (__main__.TestSelfCheckGPT) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 484.834s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f823f52c520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[\"\"], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
