<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>saga_llm_evaluation_ml.score API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>saga_llm_evaluation_ml.score</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from saga_llm_evaluation_ml.helpers.embedding_metrics import MAUVE, BERTScore
from saga_llm_evaluation_ml.helpers.language_metrics import BLEURTScore, QSquared
from saga_llm_evaluation_ml.helpers.llm_metrics import GEval, GPTScore, SelfCheckGPT
from saga_llm_evaluation_ml.helpers.utils import MetadataExtractor, get_llama_model

LLAMA_MODEL = get_llama_model()


class LLMScorer:
    # pylint: disable=invalid-name
    # pylint: disable=too-many-arguments
    def __init__(
        self,
        model=None,
        lan=&#34;en&#34;,
        bleurt_model=&#34;BLEURT-tiny&#34;,
        mauve_model=&#34;gpt2&#34;,
        selfcheckgpt_eval_model_name_or_path=&#34;TheBloke/Llama-2-7b-Chat-GGUF&#34;,
        selfcheckgpt_eval_model_basename=&#34;llama-2-7b-chat.Q4_K_M.gguf&#34;,
        geval_model_name_or_path=&#34;TheBloke/Llama-2-7b-Chat-GGUF&#34;,
        geval_model_basename=&#34;llama-2-7b-chat.Q4_K_M.gguf&#34;,
        gptscore_model_name_or_path=&#34;TheBloke/Llama-2-7b-Chat-GGUF&#34;,
        gptscore_model_basename=&#34;llama-2-7b-chat.Q4_K_M.gguf&#34;,
    ) -&gt; None:
        assert isinstance(lan, str), &#34;lan must be a string.&#34;
        assert isinstance(bleurt_model, str), &#34;bleurt_model must be a string.&#34;
        assert isinstance(mauve_model, str), &#34;mauve_model must be a string.&#34;
        assert isinstance(
            selfcheckgpt_eval_model_name_or_path, str
        ), &#34;selfcheckgpt_eval_model_name_or_path must be a string.&#34;
        assert isinstance(
            selfcheckgpt_eval_model_basename, str
        ), &#34;selfcheckgpt_eval_model_basename must be a string.&#34;
        assert isinstance(
            geval_model_name_or_path, str
        ), &#34;geval_model_name_or_path must be a string.&#34;
        assert isinstance(
            geval_model_basename, str
        ), &#34;geval_model_basename must be a string.&#34;
        assert isinstance(
            gptscore_model_name_or_path, str
        ), &#34;gptscore_model_name_or_path must be a string.&#34;
        assert isinstance(
            gptscore_model_basename, str
        ), &#34;gptscore_model_basename must be a string.&#34;

        # Metrics
        self.bert_score = BERTScore(lan=lan)
        self.mauve = MAUVE(featurize_model_name=mauve_model)
        self.bleurt_score = BLEURTScore(checkpoint=bleurt_model)
        self.q_squared = QSquared(lan=lan)
        self.selfcheckgpt = (
            None
            if model is None
            else SelfCheckGPT(
                model,
                eval_model=LLAMA_MODEL,
                eval_model_name_or_path=selfcheckgpt_eval_model_name_or_path,
                eval_model_basename=selfcheckgpt_eval_model_basename,
            )
        )
        self.geval = GEval(
            model=LLAMA_MODEL,
            model_name_or_path=geval_model_name_or_path,
            model_basename=geval_model_basename,
        )
        self.gptscore = GPTScore(
            model=LLAMA_MODEL,
            model_name_or_path=gptscore_model_name_or_path,
            model_basename=gptscore_model_basename,
        )

        # Metadata
        self.metadata_extractor = MetadataExtractor()

    def add_geval_task(self, name, definition):
        &#34;&#34;&#34;
        This function adds a task to the GEval metric.
        Please try to follow the following example pattern to ensure consistency.
        Example:
        &#34;summ&#34;: &#34;You will be given one summary written for a news article.
            Your task is to rate the summary on one metric.
            Please make sure you read and understand these instructions carefully.
            Please keep this document open while reviewing, and refer to it as needed.&#34;,
        Args:
            name (str): Name of the task.
            definition (str): Definition of the task.
        &#34;&#34;&#34;
        assert isinstance(name, str), &#34;name must be a string.&#34;
        assert isinstance(definition, str), &#34;definition must be a string.&#34;

        if self.geval is not None:
            self.geval.add_task(name, definition)
        else:
            raise TypeError(&#34;GEval metric is not defined.&#34;)

    def add_geval_aspect(self, code, name, prompt):
        &#34;&#34;&#34;
        This function adds an aspect to the GEval metric.
        Please try to follow the following example pattern to ensure consistency.
        Example:
        &#34;COH&#34;: {
            &#34;name&#34;: &#34;Coherence&#34;,
            &#34;prompt&#34;: &#34;Coherence (1-5) - the collective quality of all sentences.
                We align this dimension with the DUC quality question of structure and coherence
                whereby ”the summary should be well-structured and well-organized.
                The summary should not just be a heap of related information,
                but should build from sentence to sentence to a coherent body of information about a topic.”&#34;,
        },

        Args:
            code (str): Code of the aspect.
            name (str): Name of the aspect.
            prompt (str): Prompt of the aspect.
        &#34;&#34;&#34;
        assert isinstance(code, str), &#34;code must be a string.&#34;
        assert isinstance(name, str), &#34;name must be a string.&#34;
        assert isinstance(prompt, str), &#34;prompt must be a string.&#34;

        if self.geval is not None:
            self.geval.add_aspect(code, name, prompt)
        else:
            raise TypeError(&#34;GEval metric is not defined.&#34;)

    def add_gptscore_template(self, task, code, prompt):
        &#34;&#34;&#34;
        This function adds a template to the GPTScore metric.
        Please try to follow the following example pattern to ensure consistency.
        Example:
        &#34;diag&#34;: {
            &#34;COH&#34;: f&#34;Answer the question based on the conversation between a human and AI.
            \nQuestion: Is the AI coherent and maintains a good conversation flow throughout the conversation?
            (a) Yes. (b) No.\nConversation:\nUser: {{src}}\nAI: {{pred}}\nAnswer:&#34;,
        }
        Args:
            task (str): Task of the template.
            code (str): Code of the aspect.
            prompt (str): Prompt of the aspect.
        &#34;&#34;&#34;
        assert isinstance(task, str), &#34;task must be a string.&#34;
        assert isinstance(code, str), &#34;code must be a string.&#34;
        assert isinstance(prompt, str), &#34;prompt must be a string.&#34;

        if self.gptscore is not None:
            self.gptscore.add_template(task, code, prompt)
        else:
            raise TypeError(&#34;GPTScore metric is not defined.&#34;)

    def score(
        self,
        llm_input: str,
        prompt: str,
        prediction: str,
        context: str = None,
        reference: str = None,
        n_samples: int = 5,
        task: str = None,
        aspects: list = None,
        custom_prompt: dict = None,
    ):
        &#34;&#34;&#34;
        Args:
            llm_input (str): llm_input to the model.
            prompt (str): Prompt to the model. Comprises the context and the llm_input.
            prediction (str): Prediction of the model.
            context (str, optional): Context of the prediction. Defaults to None.
            reference (str, optional): Reference of the prediction. Defaults to None.
            n_samples (int, optional): Number of samples to generate. Defaults to 5.
            task (str, optional): Task definition. Defaults to None.
            aspects (list, optional): Aspects to evaluate. Defaults to None.
            custom_prompt (dict, optional): Custom prompt template. Defaults to None.
                Must contain the following keys: &#34;task&#34;, &#34;aspect&#34;, &#34;name&#34;.
        &#34;&#34;&#34;
        assert isinstance(prompt, str), &#34;prompt must be a string.&#34;
        assert isinstance(llm_input, str), &#34;llm_input must be a string.&#34;
        assert isinstance(prediction, str), &#34;prediction must be a string.&#34;
        assert isinstance(context, str) or context is None, &#34;context must be a string.&#34;
        assert (
            isinstance(reference, str) or reference is None
        ), &#34;Reference must be a string or None.&#34;
        assert isinstance(n_samples, int), &#34;n_samples must be an integer.&#34;
        assert n_samples &gt; 0, &#34;n_samples must be greater than 0.&#34;
        assert isinstance(task, str) or task is None, &#34;task must be a string or None.&#34;
        assert (
            isinstance(aspects, list) or aspects is None
        ), &#34;aspects must be a list or None.&#34;
        assert (
            isinstance(custom_prompt, dict) or custom_prompt is None
        ), &#34;custom_prompt must be a dict or None.&#34;
        if isinstance(custom_prompt, dict):
            assert (
                &#34;task&#34; in custom_prompt.keys()
                and &#34;aspect&#34; in custom_prompt.keys()
                and &#34;name&#34; in custom_prompt.keys()
            ), &#34;custom_prompt must contain the following keys: &#39;task&#39;, &#39;aspect&#39;, &#39;name&#39;.&#34;

        if aspects:
            geval_scores = {}
            gpt_scores = {}
            for aspect in aspects:
                geval_scores[aspect] = self.geval.compute(
                    prompt, prediction, task, aspect, custom_prompt
                )
                gpt_scores[aspect] = self.gptscore.compute(
                    prompt, prediction, custom_prompt, aspect, task
                )

        metadata_dict = {
            &#34;prompt&#34;: self.metadata_extractor.compute(prompt),
            &#34;llm_input&#34;: self.metadata_extractor.compute(llm_input),
            &#34;prediction&#34;: self.metadata_extractor.compute(prediction),
            &#34;context&#34;: self.metadata_extractor.compute(context) if context else None,
            &#34;reference&#34;: self.metadata_extractor.compute(reference)
            if reference
            else None,
        }

        metrics_dict = {
            &#34;bert_score&#34;: self.bert_score.compute([reference], [prediction])
            if reference
            else None,
            &#34;mauve&#34;: self.mauve.compute([reference], [prediction])
            if reference
            else None,
            &#34;bleurt_score&#34;: self.bleurt_score.compute([reference], [prediction])
            if reference
            else None,
            &#34;q_squared&#34;: self.q_squared.compute(prediction, context),
            &#34;selfcheck_gpt&#34;: None
            if self.selfcheckgpt is None
            else self.selfcheckgpt.compute(llm_input, prediction, n_samples),
            &#34;g_eval&#34;: self.geval.compute(
                prompt, prediction, custom_prompt=custom_prompt
            )
            if custom_prompt
            else geval_scores
            if aspects and task
            else None,
            &#34;gpt_score&#34;: self.gptscore.compute(prompt, prediction, prompt=custom_prompt)
            if custom_prompt
            else gpt_scores
            if aspects and task
            else None,
        }

        return {&#34;metadata&#34;: metadata_dict, &#34;metrics&#34;: metrics_dict}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="saga_llm_evaluation_ml.score.LLMScorer"><code class="flex name class">
<span>class <span class="ident">LLMScorer</span></span>
<span>(</span><span>model=None, lan='en', bleurt_model='BLEURT-tiny', mauve_model='gpt2', selfcheckgpt_eval_model_name_or_path='TheBloke/Llama-2-7b-Chat-GGUF', selfcheckgpt_eval_model_basename='llama-2-7b-chat.Q4_K_M.gguf', geval_model_name_or_path='TheBloke/Llama-2-7b-Chat-GGUF', geval_model_basename='llama-2-7b-chat.Q4_K_M.gguf', gptscore_model_name_or_path='TheBloke/Llama-2-7b-Chat-GGUF', gptscore_model_basename='llama-2-7b-chat.Q4_K_M.gguf')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLMScorer:
    # pylint: disable=invalid-name
    # pylint: disable=too-many-arguments
    def __init__(
        self,
        model=None,
        lan=&#34;en&#34;,
        bleurt_model=&#34;BLEURT-tiny&#34;,
        mauve_model=&#34;gpt2&#34;,
        selfcheckgpt_eval_model_name_or_path=&#34;TheBloke/Llama-2-7b-Chat-GGUF&#34;,
        selfcheckgpt_eval_model_basename=&#34;llama-2-7b-chat.Q4_K_M.gguf&#34;,
        geval_model_name_or_path=&#34;TheBloke/Llama-2-7b-Chat-GGUF&#34;,
        geval_model_basename=&#34;llama-2-7b-chat.Q4_K_M.gguf&#34;,
        gptscore_model_name_or_path=&#34;TheBloke/Llama-2-7b-Chat-GGUF&#34;,
        gptscore_model_basename=&#34;llama-2-7b-chat.Q4_K_M.gguf&#34;,
    ) -&gt; None:
        assert isinstance(lan, str), &#34;lan must be a string.&#34;
        assert isinstance(bleurt_model, str), &#34;bleurt_model must be a string.&#34;
        assert isinstance(mauve_model, str), &#34;mauve_model must be a string.&#34;
        assert isinstance(
            selfcheckgpt_eval_model_name_or_path, str
        ), &#34;selfcheckgpt_eval_model_name_or_path must be a string.&#34;
        assert isinstance(
            selfcheckgpt_eval_model_basename, str
        ), &#34;selfcheckgpt_eval_model_basename must be a string.&#34;
        assert isinstance(
            geval_model_name_or_path, str
        ), &#34;geval_model_name_or_path must be a string.&#34;
        assert isinstance(
            geval_model_basename, str
        ), &#34;geval_model_basename must be a string.&#34;
        assert isinstance(
            gptscore_model_name_or_path, str
        ), &#34;gptscore_model_name_or_path must be a string.&#34;
        assert isinstance(
            gptscore_model_basename, str
        ), &#34;gptscore_model_basename must be a string.&#34;

        # Metrics
        self.bert_score = BERTScore(lan=lan)
        self.mauve = MAUVE(featurize_model_name=mauve_model)
        self.bleurt_score = BLEURTScore(checkpoint=bleurt_model)
        self.q_squared = QSquared(lan=lan)
        self.selfcheckgpt = (
            None
            if model is None
            else SelfCheckGPT(
                model,
                eval_model=LLAMA_MODEL,
                eval_model_name_or_path=selfcheckgpt_eval_model_name_or_path,
                eval_model_basename=selfcheckgpt_eval_model_basename,
            )
        )
        self.geval = GEval(
            model=LLAMA_MODEL,
            model_name_or_path=geval_model_name_or_path,
            model_basename=geval_model_basename,
        )
        self.gptscore = GPTScore(
            model=LLAMA_MODEL,
            model_name_or_path=gptscore_model_name_or_path,
            model_basename=gptscore_model_basename,
        )

        # Metadata
        self.metadata_extractor = MetadataExtractor()

    def add_geval_task(self, name, definition):
        &#34;&#34;&#34;
        This function adds a task to the GEval metric.
        Please try to follow the following example pattern to ensure consistency.
        Example:
        &#34;summ&#34;: &#34;You will be given one summary written for a news article.
            Your task is to rate the summary on one metric.
            Please make sure you read and understand these instructions carefully.
            Please keep this document open while reviewing, and refer to it as needed.&#34;,
        Args:
            name (str): Name of the task.
            definition (str): Definition of the task.
        &#34;&#34;&#34;
        assert isinstance(name, str), &#34;name must be a string.&#34;
        assert isinstance(definition, str), &#34;definition must be a string.&#34;

        if self.geval is not None:
            self.geval.add_task(name, definition)
        else:
            raise TypeError(&#34;GEval metric is not defined.&#34;)

    def add_geval_aspect(self, code, name, prompt):
        &#34;&#34;&#34;
        This function adds an aspect to the GEval metric.
        Please try to follow the following example pattern to ensure consistency.
        Example:
        &#34;COH&#34;: {
            &#34;name&#34;: &#34;Coherence&#34;,
            &#34;prompt&#34;: &#34;Coherence (1-5) - the collective quality of all sentences.
                We align this dimension with the DUC quality question of structure and coherence
                whereby ”the summary should be well-structured and well-organized.
                The summary should not just be a heap of related information,
                but should build from sentence to sentence to a coherent body of information about a topic.”&#34;,
        },

        Args:
            code (str): Code of the aspect.
            name (str): Name of the aspect.
            prompt (str): Prompt of the aspect.
        &#34;&#34;&#34;
        assert isinstance(code, str), &#34;code must be a string.&#34;
        assert isinstance(name, str), &#34;name must be a string.&#34;
        assert isinstance(prompt, str), &#34;prompt must be a string.&#34;

        if self.geval is not None:
            self.geval.add_aspect(code, name, prompt)
        else:
            raise TypeError(&#34;GEval metric is not defined.&#34;)

    def add_gptscore_template(self, task, code, prompt):
        &#34;&#34;&#34;
        This function adds a template to the GPTScore metric.
        Please try to follow the following example pattern to ensure consistency.
        Example:
        &#34;diag&#34;: {
            &#34;COH&#34;: f&#34;Answer the question based on the conversation between a human and AI.
            \nQuestion: Is the AI coherent and maintains a good conversation flow throughout the conversation?
            (a) Yes. (b) No.\nConversation:\nUser: {{src}}\nAI: {{pred}}\nAnswer:&#34;,
        }
        Args:
            task (str): Task of the template.
            code (str): Code of the aspect.
            prompt (str): Prompt of the aspect.
        &#34;&#34;&#34;
        assert isinstance(task, str), &#34;task must be a string.&#34;
        assert isinstance(code, str), &#34;code must be a string.&#34;
        assert isinstance(prompt, str), &#34;prompt must be a string.&#34;

        if self.gptscore is not None:
            self.gptscore.add_template(task, code, prompt)
        else:
            raise TypeError(&#34;GPTScore metric is not defined.&#34;)

    def score(
        self,
        llm_input: str,
        prompt: str,
        prediction: str,
        context: str = None,
        reference: str = None,
        n_samples: int = 5,
        task: str = None,
        aspects: list = None,
        custom_prompt: dict = None,
    ):
        &#34;&#34;&#34;
        Args:
            llm_input (str): llm_input to the model.
            prompt (str): Prompt to the model. Comprises the context and the llm_input.
            prediction (str): Prediction of the model.
            context (str, optional): Context of the prediction. Defaults to None.
            reference (str, optional): Reference of the prediction. Defaults to None.
            n_samples (int, optional): Number of samples to generate. Defaults to 5.
            task (str, optional): Task definition. Defaults to None.
            aspects (list, optional): Aspects to evaluate. Defaults to None.
            custom_prompt (dict, optional): Custom prompt template. Defaults to None.
                Must contain the following keys: &#34;task&#34;, &#34;aspect&#34;, &#34;name&#34;.
        &#34;&#34;&#34;
        assert isinstance(prompt, str), &#34;prompt must be a string.&#34;
        assert isinstance(llm_input, str), &#34;llm_input must be a string.&#34;
        assert isinstance(prediction, str), &#34;prediction must be a string.&#34;
        assert isinstance(context, str) or context is None, &#34;context must be a string.&#34;
        assert (
            isinstance(reference, str) or reference is None
        ), &#34;Reference must be a string or None.&#34;
        assert isinstance(n_samples, int), &#34;n_samples must be an integer.&#34;
        assert n_samples &gt; 0, &#34;n_samples must be greater than 0.&#34;
        assert isinstance(task, str) or task is None, &#34;task must be a string or None.&#34;
        assert (
            isinstance(aspects, list) or aspects is None
        ), &#34;aspects must be a list or None.&#34;
        assert (
            isinstance(custom_prompt, dict) or custom_prompt is None
        ), &#34;custom_prompt must be a dict or None.&#34;
        if isinstance(custom_prompt, dict):
            assert (
                &#34;task&#34; in custom_prompt.keys()
                and &#34;aspect&#34; in custom_prompt.keys()
                and &#34;name&#34; in custom_prompt.keys()
            ), &#34;custom_prompt must contain the following keys: &#39;task&#39;, &#39;aspect&#39;, &#39;name&#39;.&#34;

        if aspects:
            geval_scores = {}
            gpt_scores = {}
            for aspect in aspects:
                geval_scores[aspect] = self.geval.compute(
                    prompt, prediction, task, aspect, custom_prompt
                )
                gpt_scores[aspect] = self.gptscore.compute(
                    prompt, prediction, custom_prompt, aspect, task
                )

        metadata_dict = {
            &#34;prompt&#34;: self.metadata_extractor.compute(prompt),
            &#34;llm_input&#34;: self.metadata_extractor.compute(llm_input),
            &#34;prediction&#34;: self.metadata_extractor.compute(prediction),
            &#34;context&#34;: self.metadata_extractor.compute(context) if context else None,
            &#34;reference&#34;: self.metadata_extractor.compute(reference)
            if reference
            else None,
        }

        metrics_dict = {
            &#34;bert_score&#34;: self.bert_score.compute([reference], [prediction])
            if reference
            else None,
            &#34;mauve&#34;: self.mauve.compute([reference], [prediction])
            if reference
            else None,
            &#34;bleurt_score&#34;: self.bleurt_score.compute([reference], [prediction])
            if reference
            else None,
            &#34;q_squared&#34;: self.q_squared.compute(prediction, context),
            &#34;selfcheck_gpt&#34;: None
            if self.selfcheckgpt is None
            else self.selfcheckgpt.compute(llm_input, prediction, n_samples),
            &#34;g_eval&#34;: self.geval.compute(
                prompt, prediction, custom_prompt=custom_prompt
            )
            if custom_prompt
            else geval_scores
            if aspects and task
            else None,
            &#34;gpt_score&#34;: self.gptscore.compute(prompt, prediction, prompt=custom_prompt)
            if custom_prompt
            else gpt_scores
            if aspects and task
            else None,
        }

        return {&#34;metadata&#34;: metadata_dict, &#34;metrics&#34;: metrics_dict}</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="saga_llm_evaluation_ml.score.LLMScorer.add_geval_aspect"><code class="name flex">
<span>def <span class="ident">add_geval_aspect</span></span>(<span>self, code, name, prompt)</span>
</code></dt>
<dd>
<div class="desc"><p>This function adds an aspect to the GEval metric.
Please try to follow the following example pattern to ensure consistency.
Example:
"COH": {
"name": "Coherence",
"prompt": "Coherence (1-5) - the collective quality of all sentences.
We align this dimension with the DUC quality question of structure and coherence
whereby ”the summary should be well-structured and well-organized.
The summary should not just be a heap of related information,
but should build from sentence to sentence to a coherent body of information about a topic.”",
},</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>code</code></strong> :&ensp;<code>str</code></dt>
<dd>Code of the aspect.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the aspect.</dd>
<dt><strong><code>prompt</code></strong> :&ensp;<code>str</code></dt>
<dd>Prompt of the aspect.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_geval_aspect(self, code, name, prompt):
    &#34;&#34;&#34;
    This function adds an aspect to the GEval metric.
    Please try to follow the following example pattern to ensure consistency.
    Example:
    &#34;COH&#34;: {
        &#34;name&#34;: &#34;Coherence&#34;,
        &#34;prompt&#34;: &#34;Coherence (1-5) - the collective quality of all sentences.
            We align this dimension with the DUC quality question of structure and coherence
            whereby ”the summary should be well-structured and well-organized.
            The summary should not just be a heap of related information,
            but should build from sentence to sentence to a coherent body of information about a topic.”&#34;,
    },

    Args:
        code (str): Code of the aspect.
        name (str): Name of the aspect.
        prompt (str): Prompt of the aspect.
    &#34;&#34;&#34;
    assert isinstance(code, str), &#34;code must be a string.&#34;
    assert isinstance(name, str), &#34;name must be a string.&#34;
    assert isinstance(prompt, str), &#34;prompt must be a string.&#34;

    if self.geval is not None:
        self.geval.add_aspect(code, name, prompt)
    else:
        raise TypeError(&#34;GEval metric is not defined.&#34;)</code></pre>
</details>
</dd>
<dt id="saga_llm_evaluation_ml.score.LLMScorer.add_geval_task"><code class="name flex">
<span>def <span class="ident">add_geval_task</span></span>(<span>self, name, definition)</span>
</code></dt>
<dd>
<div class="desc"><p>This function adds a task to the GEval metric.
Please try to follow the following example pattern to ensure consistency.
Example:
"summ": "You will be given one summary written for a news article.
Your task is to rate the summary on one metric.
Please make sure you read and understand these instructions carefully.
Please keep this document open while reviewing, and refer to it as needed.",</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the task.</dd>
<dt><strong><code>definition</code></strong> :&ensp;<code>str</code></dt>
<dd>Definition of the task.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_geval_task(self, name, definition):
    &#34;&#34;&#34;
    This function adds a task to the GEval metric.
    Please try to follow the following example pattern to ensure consistency.
    Example:
    &#34;summ&#34;: &#34;You will be given one summary written for a news article.
        Your task is to rate the summary on one metric.
        Please make sure you read and understand these instructions carefully.
        Please keep this document open while reviewing, and refer to it as needed.&#34;,
    Args:
        name (str): Name of the task.
        definition (str): Definition of the task.
    &#34;&#34;&#34;
    assert isinstance(name, str), &#34;name must be a string.&#34;
    assert isinstance(definition, str), &#34;definition must be a string.&#34;

    if self.geval is not None:
        self.geval.add_task(name, definition)
    else:
        raise TypeError(&#34;GEval metric is not defined.&#34;)</code></pre>
</details>
</dd>
<dt id="saga_llm_evaluation_ml.score.LLMScorer.add_gptscore_template"><code class="name flex">
<span>def <span class="ident">add_gptscore_template</span></span>(<span>self, task, code, prompt)</span>
</code></dt>
<dd>
<div class="desc"><p>This function adds a template to the GPTScore metric.
Please try to follow the following example pattern to ensure consistency.
Example:
"diag": {
"COH": f"Answer the question based on the conversation between a human and AI.</p>
<p>Question: Is the AI coherent and maintains a good conversation flow throughout the conversation?
(a) Yes. (b) No.
Conversation:
User: {{src}}
AI: {{pred}}
Answer:",
}
Args:
task (str): Task of the template.
code (str): Code of the aspect.
prompt (str): Prompt of the aspect.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_gptscore_template(self, task, code, prompt):
    &#34;&#34;&#34;
    This function adds a template to the GPTScore metric.
    Please try to follow the following example pattern to ensure consistency.
    Example:
    &#34;diag&#34;: {
        &#34;COH&#34;: f&#34;Answer the question based on the conversation between a human and AI.
        \nQuestion: Is the AI coherent and maintains a good conversation flow throughout the conversation?
        (a) Yes. (b) No.\nConversation:\nUser: {{src}}\nAI: {{pred}}\nAnswer:&#34;,
    }
    Args:
        task (str): Task of the template.
        code (str): Code of the aspect.
        prompt (str): Prompt of the aspect.
    &#34;&#34;&#34;
    assert isinstance(task, str), &#34;task must be a string.&#34;
    assert isinstance(code, str), &#34;code must be a string.&#34;
    assert isinstance(prompt, str), &#34;prompt must be a string.&#34;

    if self.gptscore is not None:
        self.gptscore.add_template(task, code, prompt)
    else:
        raise TypeError(&#34;GPTScore metric is not defined.&#34;)</code></pre>
</details>
</dd>
<dt id="saga_llm_evaluation_ml.score.LLMScorer.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, llm_input: str, prompt: str, prediction: str, context: str = None, reference: str = None, n_samples: int = 5, task: str = None, aspects: list = None, custom_prompt: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>llm_input</code></strong> :&ensp;<code>str</code></dt>
<dd>llm_input to the model.</dd>
<dt><strong><code>prompt</code></strong> :&ensp;<code>str</code></dt>
<dd>Prompt to the model. Comprises the context and the llm_input.</dd>
<dt><strong><code>prediction</code></strong> :&ensp;<code>str</code></dt>
<dd>Prediction of the model.</dd>
<dt><strong><code>context</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Context of the prediction. Defaults to None.</dd>
<dt><strong><code>reference</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Reference of the prediction. Defaults to None.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of samples to generate. Defaults to 5.</dd>
<dt><strong><code>task</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Task definition. Defaults to None.</dd>
<dt><strong><code>aspects</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Aspects to evaluate. Defaults to None.</dd>
<dt><strong><code>custom_prompt</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Custom prompt template. Defaults to None.
Must contain the following keys: "task", "aspect", "name".</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(
    self,
    llm_input: str,
    prompt: str,
    prediction: str,
    context: str = None,
    reference: str = None,
    n_samples: int = 5,
    task: str = None,
    aspects: list = None,
    custom_prompt: dict = None,
):
    &#34;&#34;&#34;
    Args:
        llm_input (str): llm_input to the model.
        prompt (str): Prompt to the model. Comprises the context and the llm_input.
        prediction (str): Prediction of the model.
        context (str, optional): Context of the prediction. Defaults to None.
        reference (str, optional): Reference of the prediction. Defaults to None.
        n_samples (int, optional): Number of samples to generate. Defaults to 5.
        task (str, optional): Task definition. Defaults to None.
        aspects (list, optional): Aspects to evaluate. Defaults to None.
        custom_prompt (dict, optional): Custom prompt template. Defaults to None.
            Must contain the following keys: &#34;task&#34;, &#34;aspect&#34;, &#34;name&#34;.
    &#34;&#34;&#34;
    assert isinstance(prompt, str), &#34;prompt must be a string.&#34;
    assert isinstance(llm_input, str), &#34;llm_input must be a string.&#34;
    assert isinstance(prediction, str), &#34;prediction must be a string.&#34;
    assert isinstance(context, str) or context is None, &#34;context must be a string.&#34;
    assert (
        isinstance(reference, str) or reference is None
    ), &#34;Reference must be a string or None.&#34;
    assert isinstance(n_samples, int), &#34;n_samples must be an integer.&#34;
    assert n_samples &gt; 0, &#34;n_samples must be greater than 0.&#34;
    assert isinstance(task, str) or task is None, &#34;task must be a string or None.&#34;
    assert (
        isinstance(aspects, list) or aspects is None
    ), &#34;aspects must be a list or None.&#34;
    assert (
        isinstance(custom_prompt, dict) or custom_prompt is None
    ), &#34;custom_prompt must be a dict or None.&#34;
    if isinstance(custom_prompt, dict):
        assert (
            &#34;task&#34; in custom_prompt.keys()
            and &#34;aspect&#34; in custom_prompt.keys()
            and &#34;name&#34; in custom_prompt.keys()
        ), &#34;custom_prompt must contain the following keys: &#39;task&#39;, &#39;aspect&#39;, &#39;name&#39;.&#34;

    if aspects:
        geval_scores = {}
        gpt_scores = {}
        for aspect in aspects:
            geval_scores[aspect] = self.geval.compute(
                prompt, prediction, task, aspect, custom_prompt
            )
            gpt_scores[aspect] = self.gptscore.compute(
                prompt, prediction, custom_prompt, aspect, task
            )

    metadata_dict = {
        &#34;prompt&#34;: self.metadata_extractor.compute(prompt),
        &#34;llm_input&#34;: self.metadata_extractor.compute(llm_input),
        &#34;prediction&#34;: self.metadata_extractor.compute(prediction),
        &#34;context&#34;: self.metadata_extractor.compute(context) if context else None,
        &#34;reference&#34;: self.metadata_extractor.compute(reference)
        if reference
        else None,
    }

    metrics_dict = {
        &#34;bert_score&#34;: self.bert_score.compute([reference], [prediction])
        if reference
        else None,
        &#34;mauve&#34;: self.mauve.compute([reference], [prediction])
        if reference
        else None,
        &#34;bleurt_score&#34;: self.bleurt_score.compute([reference], [prediction])
        if reference
        else None,
        &#34;q_squared&#34;: self.q_squared.compute(prediction, context),
        &#34;selfcheck_gpt&#34;: None
        if self.selfcheckgpt is None
        else self.selfcheckgpt.compute(llm_input, prediction, n_samples),
        &#34;g_eval&#34;: self.geval.compute(
            prompt, prediction, custom_prompt=custom_prompt
        )
        if custom_prompt
        else geval_scores
        if aspects and task
        else None,
        &#34;gpt_score&#34;: self.gptscore.compute(prompt, prediction, prompt=custom_prompt)
        if custom_prompt
        else gpt_scores
        if aspects and task
        else None,
    }

    return {&#34;metadata&#34;: metadata_dict, &#34;metrics&#34;: metrics_dict}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="saga_llm_evaluation_ml" href="index.html">saga_llm_evaluation_ml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="saga_llm_evaluation_ml.score.LLMScorer" href="#saga_llm_evaluation_ml.score.LLMScorer">LLMScorer</a></code></h4>
<ul class="">
<li><code><a title="saga_llm_evaluation_ml.score.LLMScorer.add_geval_aspect" href="#saga_llm_evaluation_ml.score.LLMScorer.add_geval_aspect">add_geval_aspect</a></code></li>
<li><code><a title="saga_llm_evaluation_ml.score.LLMScorer.add_geval_task" href="#saga_llm_evaluation_ml.score.LLMScorer.add_geval_task">add_geval_task</a></code></li>
<li><code><a title="saga_llm_evaluation_ml.score.LLMScorer.add_gptscore_template" href="#saga_llm_evaluation_ml.score.LLMScorer.add_gptscore_template">add_gptscore_template</a></code></li>
<li><code><a title="saga_llm_evaluation_ml.score.LLMScorer.score" href="#saga_llm_evaluation_ml.score.LLMScorer.score">score</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>