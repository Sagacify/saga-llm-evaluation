Module saga_llm_evaluation_ml.helpers.language_metrics
======================================================

Classes
-------

`BLEURTScore(checkpoint='BLEURT-tiny')`
:   BLEURT is a learnt metric that uses BERT to compute a similarity score for each token
    in the candidate sentence with each token in the reference sentence.
    
    Args:
        checkpoint (str, optional): Checkpoint to use. Defaults to BLEURT-tiny if not specified.

    ### Methods

    `compute(self, references, predictions, **kwargs)`
    :   Args:
            references (list): List of reference sentences.
            predictions (list): List of candidate sentences.
        
        Returns:
            list: List of scores for each candidate sentence.

`QSquared(lan='en')`
:   Q² is a reference-free metric that aims to evaluate the factual consistency of knowledge-grounded
    dialogue systems. The approach is based on automatic question generation and question answering
    Source: https://github.com/orhonovich/q-squared
    
    Args:
        lan (str, optional): Language to use. Defaults to "en", It may also be "fr".

    ### Methods

    `compute(self, response, knowledge, single=False, remove_personal=True)`
    :   Compute the Q² score for a given response and knowledge.
        Args:
            response (str) : text generated by the LLM
            knowledge (str) : knowledge given as a context to the LLM
            single (bool) : if True, only one question is generated for each candidate answer.
                            Defaults to False.
            remove_personal (bool) : if True, remove questions that contain personal pronouns.
                                     Defaults to True.
        Returns:
            avg_f1 (float) : average F1-bert-score of the knowledge answers (Q² score)

    `get_answer(self, question: str, text: str)`
    :   Search for the answer in the text given the question.
        Args:
            question (str) : question to ask
            text (str) : text to search in
        Returns:
            answer (str) : answer to the question

    `get_answer_candidates(self, text: str)`
    :   Look for candidate aswers that could be answered by the text.
        Args:
            text (str) : text to search in
        Returns:
            candidates (str) : candidates answers

    `get_questions_beam(self, answer, context, max_length=128, beam_size=5, num_return=5)`
    :   Get the n best questions for a given answer, given the context. "Beam" is the name of the
        approach
        Args:
            answer (str) : answer to the question
            context (str) : context to search in
            max_length (int, optional) : max length of the generated question. Defaults to 128.
            beam_size (int, optional) : beam size. Defaults to 5.
            num_return (int, optional) : number of questions to return. Defaults to 5.
        Returns:
            all_questions (list) : n best questions

    `single_question_score(self, question, answer, response, knowledge)`
    :   Given a candidate pair of question and answer (generated from the candidate text), get the
        score of the aswer given by taking as a context the knowledge that the LLM was given.
        The higher the F1-score, the more the model we are trying to evaluate is consistent
        with the knowledge.
        Args:
            question (str) : cadidate question (generated from the candidate text)
            answer (str) : candidate answer (generated from the candidate text)
            response (str) : text generated by the LLM
            knowledge (str) : knowledge given as a context to the LLM
        
        Returns:
            score, answer (tuple) : bert-score of the knowledge answer, knowledge answer